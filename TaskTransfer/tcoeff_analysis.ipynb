{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SNA0cIs4wGuY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pdb\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnCvyey8wGua",
        "tags": [],
        "user_expressions": []
      },
      "source": [
        "## What this notebook contains\n",
        "\n",
        "The goal of this notebook is to set-up one of the key experiments contained in my paper. The overall idea is that even when the ground-truth data is generated as a linear regression, the alternating minimization-grad descent scheme is very suboptimal, even when you overparameterize the representation as a powerful neural network (compared to the ground truth representation, which is just a matrix in this case).\n",
        "\n",
        "To put some math into this, we assume that our data is generated as:\n",
        "\\begin{align*}\n",
        "    y^{(t)}_i = F^{(t)}_\\star \\Phi_\\star x^{(t)}_i + w^{(t)}_i,\\quad y^{(t)}_i \\in \\mathbb R^{d_y}, x^{(t)}_i \\in \\mathbb R^{d_x}, \\Phi_\\star \\in \\mathbb R^{r \\times d_x}.\n",
        "\\end{align*}\n",
        "where $t$ indexes distinct tasks $t = 1,\\dots, T$. For normalization purposes, we assume $\\Phi_\\star$ has orthonormal rows. In this set of experiments, we simplify things and assume $x^{(t)}_i$ and $w^{(t)}_i$ are independent and identically distributed across all tasks:\n",
        "\\begin{align*}\n",
        "    x_i^{(t)} \\sim \\mathcal N (0, \\Sigma_x),\\;\\; w_i^{(t)} \\sim \\mathcal N(0, \\sigma^2 \\cdot \\mathrm{Id}), \\quad \\text{for all }i,t.\n",
        "\\end{align*}\n",
        "Notably, $\\Sigma_x \\succ 0$ is going to be generated as some covariance matrix that is *non-isotropic*, i.e. not Identity (or any constant scaling of it. We can take a peek at the data generation functions we imported: `from linreg_data import generate_parameters, generate_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4NB8FGEowGua",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def generate_parameters(dx, dy, r, num_tasks):\n",
        "\n",
        "    # randomly generate covariance of x\n",
        "    U = 5*torch.eye(dx) + torch.randn(dx, dx)\n",
        "    Sigma_x = 0.5*(U + U.T)                                # Make Sigma_x symmetric\n",
        "    Sigma_x = (dx / torch.trace(Sigma_x)) * Sigma_x        # normalize to get Tr(Sigma_x) = dx\n",
        "\n",
        "    # randomly generate representation Phi*\n",
        "    A = torch.randn(r, dx)\n",
        "    _, _, Phi = torch.linalg.svd(A, full_matrices = False) # Generate Phi* as random matrix that is then orthonormalized.\n",
        "\n",
        "    # randomly generate heads F0 (for validation), F1,...,FT (for training)\n",
        "    F0 = torch.randn(dy, r)\n",
        "    Fs = []\n",
        "    Fs.append(F0)\n",
        "    for i in range(num_tasks):\n",
        "        gamma = 0.01\n",
        "        B = torch.randn(dy, dy)\n",
        "        rot = torch.linalg.matrix_exp(0.5*gamma*(B - B.T)) # Generate heads by randomly applying a rotation to F0\n",
        "        F = rot @ F0\n",
        "        Fs.append(F)\n",
        "\n",
        "    return Fs, Phi, Sigma_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDahiO3TwGub",
        "tags": [],
        "user_expressions": []
      },
      "source": [
        "As one can glean above, the `generate_parameters` is an exceedingly simple way to randomly generate the key parameters of the ground-truth data, which are the heads $F^{(1)},\\dots, F^{(T)}$, the representation $\\Phi_\\star$ and the *non-isotropic* data covariance $\\Sigma_x$.\n",
        "\n",
        "The following function `generate_data` simply generates data $(x^{(t)}_i, y_i^{(t)})$ from said parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NzXDsnNUwGub"
      },
      "outputs": [],
      "source": [
        "# def generate_data(n_points, Fs, Phi, cov_x, mode = 'train'):\n",
        "\n",
        "#     U, S, V = torch.linalg.svd(cov_x)\n",
        "#     cov_x_sqrt = U @ torch.diag(torch.sqrt(S)) @ V\n",
        "#     dy = Fs[0].shape[0]\n",
        "#     dx = Phi.shape[-1]\n",
        "\n",
        "#     if mode == 'train':\n",
        "#         X = []\n",
        "#         Y = []\n",
        "#         for i in range(1, len(Fs)):                       # for each task 1,...,T\n",
        "#             M = Fs[i] @ Phi\n",
        "#             xs = cov_x_sqrt @ torch.randn(dx, n_points)\n",
        "#             ws = 0.1*torch.randn(dy, n_points)\n",
        "#             ys = M @ xs + ws\n",
        "\n",
        "#             X.append(xs)\n",
        "#             Y.append(ys)\n",
        "#         return X, Y\n",
        "\n",
        "#     elif mode == 'test':\n",
        "#         M = Fs[0] @ Phi\n",
        "#         xs = cov_x_sqrt @ torch.randn(dx, n_points)\n",
        "#         ws = torch.randn(dy, n_points)\n",
        "#         ys = M @ xs + ws\n",
        "\n",
        "#         return xs, ys\n",
        "\n",
        "\n",
        "def generate_data(n_points, Fs, Phi, cov_x):\n",
        "    U, S, V = torch.linalg.svd(cov_x)\n",
        "    cov_x_sqrt = U @ torch.diag(torch.sqrt(S)) @ V\n",
        "    dy = Fs[0].shape[0]\n",
        "    dx = Phi.shape[-1]\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test, Ws = [], [], [], [], []\n",
        "    for i in range(len(Fs)):\n",
        "        M = Fs[i] @ Phi\n",
        "        xs = cov_x_sqrt @ torch.randn(dx, n_points)\n",
        "        ws = 0.1 * torch.randn(dy, n_points)\n",
        "        ys = M @ xs + ws\n",
        "\n",
        "        train_split = int(0.8 * n_points)\n",
        "        X_train.append(xs[:, :train_split])\n",
        "        Y_train.append(ys[:, :train_split])\n",
        "        X_test.append(xs[:, train_split:])\n",
        "        Y_test.append(ys[:, train_split:])\n",
        "        Ws.append(ws)\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test, Ws"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXVWYfSQwGub",
        "tags": [],
        "user_expressions": []
      },
      "source": [
        "The only trick we used here is a property of Gaussian random vectors: $x \\sim \\mathcal N(0, \\Sigma_x)$ is equivalent to $x = \\Sigma_x^{1/2} v$, $v \\sim \\mathcal N(0, \\mathrm{Id})$.\n",
        "\n",
        "<!-- With parameter generation and data generation in hand, let us describe the experiment. For each batch of data, we run 4 algorithms/models, all falling under the alternating minimization-descent paradigm. By this we mean, for each update iteration:\n",
        "\n",
        "1. **(Minimization)** Hold current rep. $\\hat\\Phi(\\cdot)$ fixed. For each task compute the (rep-conditioned) least-squares head:\n",
        "\\begin{align*}\n",
        "\\hat F^{(t)} = \\argmin_F \\sum_{i=1}^{\\texttt{n\\_points}} \\|y_i^{(t)} - F \\hat\\Phi (x_i^{(t)}) \\|^2.\n",
        "\\end{align*}\n",
        "2. **(Descent)** Hold the least-squares heads $\\hat F^{(1)}, \\dots, \\hat F^{(T)}$ fixed. Perform a descent step (whatever that may look like depending on the algorithm) on the representation. We usually assume this update can be computed locally for each task and averaged to yield the final update (this is certainly true for grad descent, since $\\nabla $ is a linear operator).\n",
        "\\begin{align*}\n",
        "d^{(t)} &= \\mathrm{DescentStep}(\\hat\\Phi; (x_i^{(t)}, y_i^{(t)})_{i=1}^{\\texttt{n\\_points}},\\; \\hat F^{(t)} ), \\quad \\text{for }t = 1,\\dots, T \\\\\n",
        "\\hat\\Phi^{(t)}_{\\mathrm{next}} &= \\hat\\Phi - \\eta d^{(t)} \\\\\n",
        "\\hat\\Phi_{\\mathrm{next}} &= \\frac{1}{T}\\sum_{t=1}^T \\hat\\Phi^{(t)}_{\\mathrm{next}} \\\\\n",
        "\\text{(For certain methods) }\\hat\\Phi_{\\mathrm{next}} &= \\mathrm{Postprocess}(\\hat\\Phi_{\\mathrm{next}}).\n",
        "\\end{align*} -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-IOPxDK_wGub"
      },
      "outputs": [],
      "source": [
        "# # training loop params\n",
        "# n_experiments = 20                # number of iterations we run the pipeline (for error bars)\n",
        "# num_iter = 4000                     # number of descent steps\n",
        "\n",
        "# dy = 15\n",
        "# dx = 50\n",
        "# r = 5\n",
        "# num_tasks = 10\n",
        "# n_points = 200\n",
        "\n",
        "# DFW_lr = 0.01\n",
        "# AMDD_lr = 0.01\n",
        "# MLP_lr = 0.01\n",
        "# d_hidden = 64                      # note this is > dy, dx\n",
        "\n",
        "# DFW_all = []\n",
        "# AMGD_all = []                      # AMGD stands for alternating-minimization gradient-descent\n",
        "# AMDD_all = []                      # AMDD stands for alternating-minimization double-descent\n",
        "# alt_mlp1_all = []\n",
        "# alt_mlp2_all = []\n",
        "# optimal_loss_all = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pfsFbqPPwGuc"
      },
      "outputs": [],
      "source": [
        "#Phi is parameterized by a 2 layer nn\n",
        "class PhiNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
        "        super(PhiNet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PWosgKd1wGuc"
      },
      "outputs": [],
      "source": [
        "def least_squares_head(X, Y, Phi):\n",
        "    with torch.no_grad():\n",
        "        Phi_X = Phi(X)\n",
        "    l = Y.T @ Phi_X\n",
        "    r = torch.linalg.inv(Phi_X.T @ Phi_X + 1e-5 * torch.eye(Phi_X.shape[1], device=device))\n",
        "    return l @ r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9NKyxLTrwGuc"
      },
      "outputs": [],
      "source": [
        "def excess_risk(X, Y, F, Phi, W):\n",
        "    with torch.no_grad():\n",
        "        pred = F @ Phi(X).T\n",
        "        pred_risk = torch.mean((Y - pred.T) ** 2)\n",
        "\n",
        "        # used closed form instead\n",
        "        true_risk = (W ** 2).mean()\n",
        "        print(f\"Pred Risk: {pred_risk.item()}, True Risk: {true_risk.item()}\")\n",
        "    return pred_risk - true_risk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SCrOA-C_wGuc"
      },
      "outputs": [],
      "source": [
        "def experiment(dx, dy, r, T, n, lr, num_steps):\n",
        "    # Generate parameters and data\n",
        "    F_star, Phi_star, Sigma_x = generate_parameters(dx, dy, r, T + 1)\n",
        "    X_train, Y_train, X_test, Y_test, Ws = generate_data(n, F_star, Phi_star, Sigma_x)\n",
        "\n",
        "    # Separate hold-out task\n",
        "    X_holdout_train, Y_holdout_train = X_train[0], Y_train[0]\n",
        "    X_holdout_test, Y_holdout_test = X_test[0], Y_test[0]\n",
        "\n",
        "    # Move all data and models to device\n",
        "    F_star = [f.to(device) for f in F_star]\n",
        "    Phi_star = Phi_star.to(device)\n",
        "    X_train = [x.to(device) for x in X_train]  # Exclude the held-out task\n",
        "    Y_train = [y.to(device) for y in Y_train]\n",
        "    X_test = [x.to(device) for x in X_test]\n",
        "    Y_test = [y.to(device) for y in Y_test]\n",
        "\n",
        "    X_holdout_train, Y_holdout_train = X_holdout_train.to(device), Y_holdout_train.to(device)\n",
        "    X_holdout_test, Y_holdout_test = X_holdout_test.to(device), Y_holdout_test.to(device)\n",
        "\n",
        "    # Initialize model\n",
        "    Phi = PhiNet(input_dim=dx, output_dim=r).to(device)\n",
        "    optimizer = optim.SGD(Phi.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Train on training tasks\n",
        "    for step in range(num_steps):\n",
        "        total_loss = 0.0\n",
        "        for t in range(1, T + 1):\n",
        "            x = X_train[t].T\n",
        "            y = Y_train[t].T\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            embeddings = Phi(x)\n",
        "            F_t = least_squares_head(x, y, Phi)\n",
        "            preds = embeddings @ F_t.T\n",
        "            loss = criterion(preds, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "    # Compute excess risks\n",
        "    train_excess_risks = []\n",
        "    for t in range(1, T + 1):\n",
        "        x = X_test[t].T\n",
        "        y = Y_test[t].T\n",
        "        F_t = least_squares_head(x, y, Phi)\n",
        "        train_excess_risks.append(excess_risk(x, y, F_t, Phi, Ws[t]))\n",
        "    avg_train_excess_risk = torch.tensor(train_excess_risks, device=device).mean().item()\n",
        "\n",
        "    # Held-out task evaluation\n",
        "    F_heldout = least_squares_head(X_holdout_train.T, Y_holdout_train.T, Phi)\n",
        "    test_excess_risk = excess_risk(X_holdout_test.T, Y_holdout_test.T, F_heldout, Phi, Ws[0]).item()\n",
        "    transfer_coefficient = avg_train_excess_risk / test_excess_risk\n",
        "    print(f\"Avg Train Excess Risk: {avg_train_excess_risk}, Test Excess Risk: {test_excess_risk}\")\n",
        "    return transfer_coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5251ced233a84c37a0e4577e214c3f0f",
            "2ddae83910084e988f0de53cbe4df5e1",
            "9e9b33b2e60043aa94cd47c196d6735a",
            "85608f23de774808aa847b69fad6b511",
            "080203714a734a5fa7217982f6d8c603",
            "6d591723f791401dbbf754ddb3ada469",
            "0db33db372044da7aa990eec1b467831",
            "a40d96dfd4bb44b68db5f602739fb87f",
            "c625ed7aff794045bd56277c2da57413",
            "3c4cb02dca5d467bb386cf3f63e71169",
            "1f8ffc308a324c33bdcb07579ecd00d0"
          ]
        },
        "id": "18PvklI9wGuc",
        "outputId": "3a561d04-4f79-4e26-bbaf-2481e8be7cb5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running Experiments:   0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5251ced233a84c37a0e4577e214c3f0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred Risk: 0.076829694211483, True Risk: 0.010337019339203835\n",
            "Pred Risk: 0.06577649712562561, True Risk: 0.010114275850355625\n",
            "Pred Risk: 0.0696064829826355, True Risk: 0.009836548939347267\n",
            "Pred Risk: 0.06041308119893074, True Risk: 0.010299009270966053\n",
            "Pred Risk: 0.07395055145025253, True Risk: 0.009628577157855034\n",
            "Pred Risk: 0.05347941070795059, True Risk: 0.009183691814541817\n",
            "Pred Risk: 0.048554062843322754, True Risk: 0.010104801505804062\n",
            "Pred Risk: 0.06791109591722488, True Risk: 0.009695968590676785\n",
            "Pred Risk: 0.07891017943620682, True Risk: 0.009678086265921593\n",
            "Pred Risk: 0.07027626037597656, True Risk: 0.010443639010190964\n",
            "Pred Risk: 0.10266265273094177, True Risk: 0.009665314108133316\n",
            "Avg Train Excess Risk: 0.05663856863975525, Test Excess Risk: 0.09299734234809875\n",
            "Pred Risk: 0.05283474549651146, True Risk: 0.010284320451319218\n",
            "Pred Risk: 0.04405547305941582, True Risk: 0.009627523832023144\n",
            "Pred Risk: 0.07776662707328796, True Risk: 0.010019708424806595\n",
            "Pred Risk: 0.045783139765262604, True Risk: 0.009822972118854523\n",
            "Pred Risk: 0.03908631578087807, True Risk: 0.010203869082033634\n",
            "Pred Risk: 0.06366575509309769, True Risk: 0.010189446620643139\n",
            "Pred Risk: 0.05747772380709648, True Risk: 0.010017059743404388\n",
            "Pred Risk: 0.05763174593448639, True Risk: 0.009906776249408722\n",
            "Pred Risk: 0.0610446035861969, True Risk: 0.010961811989545822\n",
            "Pred Risk: 0.04655691981315613, True Risk: 0.010222301818430424\n",
            "Pred Risk: 0.08189424872398376, True Risk: 0.009833572432398796\n",
            "Avg Train Excess Risk: 0.044464729726314545, Test Excess Risk: 0.07206067442893982\n",
            "Pred Risk: 0.10526704043149948, True Risk: 0.009742305614054203\n",
            "Pred Risk: 0.07171785831451416, True Risk: 0.01044626533985138\n",
            "Pred Risk: 0.10966590791940689, True Risk: 0.009990781545639038\n",
            "Pred Risk: 0.08260467648506165, True Risk: 0.010364286601543427\n",
            "Pred Risk: 0.08048001676797867, True Risk: 0.009497604332864285\n",
            "Pred Risk: 0.053055424243211746, True Risk: 0.010139981284737587\n",
            "Pred Risk: 0.054356180131435394, True Risk: 0.01033161673694849\n",
            "Pred Risk: 0.07557070255279541, True Risk: 0.00982410367578268\n",
            "Pred Risk: 0.07993177324533463, True Risk: 0.009991851635277271\n",
            "Pred Risk: 0.06486949324607849, True Risk: 0.010552451945841312\n",
            "Pred Risk: 0.12591297924518585, True Risk: 0.009943695738911629\n",
            "Avg Train Excess Risk: 0.06766378879547119, Test Excess Risk: 0.11596928536891937\n",
            "Pred Risk: 0.07056023925542831, True Risk: 0.00948840007185936\n",
            "Pred Risk: 0.08683451265096664, True Risk: 0.010066308081150055\n",
            "Pred Risk: 0.07153487205505371, True Risk: 0.010557957924902439\n",
            "Pred Risk: 0.11539357155561447, True Risk: 0.009747129864990711\n",
            "Pred Risk: 0.056973692029714584, True Risk: 0.01016453467309475\n",
            "Pred Risk: 0.06337908655405045, True Risk: 0.009628526866436005\n",
            "Pred Risk: 0.05345660075545311, True Risk: 0.010233932174742222\n",
            "Pred Risk: 0.04980029910802841, True Risk: 0.009767977520823479\n",
            "Pred Risk: 0.08522773534059525, True Risk: 0.010801333002746105\n",
            "Pred Risk: 0.03928292170166969, True Risk: 0.009915214963257313\n",
            "Pred Risk: 0.12466743588447571, True Risk: 0.009948964230716228\n",
            "Avg Train Excess Risk: 0.05920722335577011, Test Excess Risk: 0.1147184744477272\n",
            "Pred Risk: 0.06182018294930458, True Risk: 0.010300899855792522\n",
            "Pred Risk: 0.07955726981163025, True Risk: 0.010085443034768105\n",
            "Pred Risk: 0.07547134160995483, True Risk: 0.010268064215779305\n",
            "Pred Risk: 0.05208105221390724, True Risk: 0.010112873278558254\n",
            "Pred Risk: 0.060381051152944565, True Risk: 0.010188603773713112\n",
            "Pred Risk: 0.04344363138079643, True Risk: 0.010225730016827583\n",
            "Pred Risk: 0.04174334183335304, True Risk: 0.010452726855874062\n",
            "Pred Risk: 0.04062337055802345, True Risk: 0.009868939407169819\n",
            "Pred Risk: 0.036775074899196625, True Risk: 0.009401080198585987\n",
            "Pred Risk: 0.05085998401045799, True Risk: 0.00939025916159153\n",
            "Pred Risk: 0.08765249699354172, True Risk: 0.010538320057094097\n",
            "Avg Train Excess Risk: 0.044246166944503784, Test Excess Risk: 0.07711417973041534\n",
            "Pred Risk: 0.07017188519239426, True Risk: 0.01029674056917429\n",
            "Pred Risk: 0.0679408386349678, True Risk: 0.009837357327342033\n",
            "Pred Risk: 0.06766798347234726, True Risk: 0.009430461563169956\n",
            "Pred Risk: 0.0588005892932415, True Risk: 0.009889975190162659\n",
            "Pred Risk: 0.0711350217461586, True Risk: 0.00928743276745081\n",
            "Pred Risk: 0.06617451459169388, True Risk: 0.010114269331097603\n",
            "Pred Risk: 0.07149207592010498, True Risk: 0.010173452086746693\n",
            "Pred Risk: 0.059624310582876205, True Risk: 0.010139493271708488\n",
            "Pred Risk: 0.07732193171977997, True Risk: 0.009660844691097736\n",
            "Pred Risk: 0.06798514723777771, True Risk: 0.009336159564554691\n",
            "Pred Risk: 0.10477504134178162, True Risk: 0.009853113442659378\n",
            "Avg Train Excess Risk: 0.05801481008529663, Test Excess Risk: 0.09492193162441254\n",
            "Pred Risk: 0.06153541058301926, True Risk: 0.009736530482769012\n",
            "Pred Risk: 0.06725221127271652, True Risk: 0.00995818991214037\n",
            "Pred Risk: 0.07731755822896957, True Risk: 0.010240187868475914\n",
            "Pred Risk: 0.0615532360970974, True Risk: 0.00988742709159851\n",
            "Pred Risk: 0.05497365444898605, True Risk: 0.010141776874661446\n",
            "Pred Risk: 0.06981023401021957, True Risk: 0.01049005426466465\n",
            "Pred Risk: 0.06764356046915054, True Risk: 0.010394314303994179\n",
            "Pred Risk: 0.0674123615026474, True Risk: 0.00953285675495863\n",
            "Pred Risk: 0.05900713428854942, True Risk: 0.009790562093257904\n",
            "Pred Risk: 0.06898817420005798, True Risk: 0.009697885252535343\n",
            "Pred Risk: 0.08687294274568558, True Risk: 0.009754538536071777\n",
            "Avg Train Excess Risk: 0.055562376976013184, Test Excess Risk: 0.0771184042096138\n",
            "Pred Risk: 0.059828631579875946, True Risk: 0.010006091557443142\n",
            "Pred Risk: 0.09687791019678116, True Risk: 0.010347912088036537\n",
            "Pred Risk: 0.055804505944252014, True Risk: 0.010463114827871323\n",
            "Pred Risk: 0.07512576878070831, True Risk: 0.010503015480935574\n",
            "Pred Risk: 0.05988625809550285, True Risk: 0.01043424941599369\n",
            "Pred Risk: 0.0672716498374939, True Risk: 0.009868045337498188\n",
            "Pred Risk: 0.06629174202680588, True Risk: 0.009887492284178734\n",
            "Pred Risk: 0.06600683182477951, True Risk: 0.009304862469434738\n",
            "Pred Risk: 0.05748086795210838, True Risk: 0.010387687012553215\n",
            "Pred Risk: 0.06563878804445267, True Risk: 0.009855619631707668\n",
            "Pred Risk: 0.11196383088827133, True Risk: 0.010264321230351925\n",
            "Avg Train Excess Risk: 0.05691548436880112, Test Excess Risk: 0.10169950872659683\n",
            "Pred Risk: 0.07821538299322128, True Risk: 0.01000662986189127\n",
            "Pred Risk: 0.06375722587108612, True Risk: 0.0098140649497509\n",
            "Pred Risk: 0.07727963477373123, True Risk: 0.009823060594499111\n",
            "Pred Risk: 0.06435693055391312, True Risk: 0.00929449126124382\n",
            "Pred Risk: 0.05990825593471527, True Risk: 0.010070162825286388\n",
            "Pred Risk: 0.059898681938648224, True Risk: 0.010011564008891582\n",
            "Pred Risk: 0.03862035647034645, True Risk: 0.010286683216691017\n",
            "Pred Risk: 0.05438534915447235, True Risk: 0.011105631478130817\n",
            "Pred Risk: 0.05024921894073486, True Risk: 0.010006137192249298\n",
            "Pred Risk: 0.06649977713823318, True Risk: 0.009382135234773159\n",
            "Pred Risk: 0.06717205047607422, True Risk: 0.009591364301741123\n",
            "Avg Train Excess Risk: 0.05133702605962753, Test Excess Risk: 0.05758068710565567\n",
            "Pred Risk: 0.0860741138458252, True Risk: 0.009927287697792053\n",
            "Pred Risk: 0.07714575529098511, True Risk: 0.009738855063915253\n",
            "Pred Risk: 0.07287738472223282, True Risk: 0.009996878914535046\n",
            "Pred Risk: 0.052949968725442886, True Risk: 0.010646033100783825\n",
            "Pred Risk: 0.07337786257266998, True Risk: 0.009905154816806316\n",
            "Pred Risk: 0.06545412540435791, True Risk: 0.010426710359752178\n",
            "Pred Risk: 0.08540129661560059, True Risk: 0.009760046377778053\n",
            "Pred Risk: 0.06274053454399109, True Risk: 0.009807907976210117\n",
            "Pred Risk: 0.056125521659851074, True Risk: 0.010272053070366383\n",
            "Pred Risk: 0.11341726779937744, True Risk: 0.009718295186758041\n",
            "Pred Risk: 0.13525551557540894, True Risk: 0.009769553318619728\n",
            "Avg Train Excess Risk: 0.06453646719455719, Test Excess Risk: 0.12548595666885376\n",
            "Pred Risk: 0.052248138934373856, True Risk: 0.010226797312498093\n",
            "Pred Risk: 0.06894588470458984, True Risk: 0.010263113304972649\n",
            "Pred Risk: 0.06350124627351761, True Risk: 0.010066188871860504\n",
            "Pred Risk: 0.051221705973148346, True Risk: 0.00982650276273489\n",
            "Pred Risk: 0.07507782429456711, True Risk: 0.008980912156403065\n",
            "Pred Risk: 0.08022895455360413, True Risk: 0.010237022303044796\n",
            "Pred Risk: 0.06705829501152039, True Risk: 0.010632417164742947\n",
            "Pred Risk: 0.06104423478245735, True Risk: 0.0095420116558671\n",
            "Pred Risk: 0.06645520031452179, True Risk: 0.010110413655638695\n",
            "Pred Risk: 0.051458872854709625, True Risk: 0.01010589674115181\n",
            "Pred Risk: 0.08811360597610474, True Risk: 0.010604430921375751\n",
            "Avg Train Excess Risk: 0.05372490733861923, Test Excess Risk: 0.07750917226076126\n",
            "Pred Risk: 0.05888325348496437, True Risk: 0.010400490835309029\n",
            "Pred Risk: 0.049136932939291, True Risk: 0.009828956797719002\n",
            "Pred Risk: 0.04657644405961037, True Risk: 0.009631454013288021\n",
            "Pred Risk: 0.06180756166577339, True Risk: 0.010794807225465775\n",
            "Pred Risk: 0.04200853034853935, True Risk: 0.00926123559474945\n",
            "Pred Risk: 0.06306745111942291, True Risk: 0.010669635608792305\n",
            "Pred Risk: 0.052936192601919174, True Risk: 0.010052756406366825\n",
            "Pred Risk: 0.047131702303886414, True Risk: 0.01018367800861597\n",
            "Pred Risk: 0.05378912016749382, True Risk: 0.009580817073583603\n",
            "Pred Risk: 0.05958275496959686, True Risk: 0.009956383146345615\n",
            "Pred Risk: 0.08275377005338669, True Risk: 0.009346112608909607\n",
            "Avg Train Excess Risk: 0.04345597326755524, Test Excess Risk: 0.07340765744447708\n",
            "Pred Risk: 0.09743352234363556, True Risk: 0.010330608114600182\n",
            "Pred Risk: 0.09732712060213089, True Risk: 0.00978040136396885\n",
            "Pred Risk: 0.05296362191438675, True Risk: 0.009554976597428322\n",
            "Pred Risk: 0.0749438926577568, True Risk: 0.009859463199973106\n",
            "Pred Risk: 0.08702036738395691, True Risk: 0.010071568191051483\n",
            "Pred Risk: 0.08555356413125992, True Risk: 0.009726885706186295\n",
            "Pred Risk: 0.0906587764620781, True Risk: 0.010181311517953873\n",
            "Pred Risk: 0.08910137414932251, True Risk: 0.00993655901402235\n",
            "Pred Risk: 0.07031915336847305, True Risk: 0.0093834288418293\n",
            "Pred Risk: 0.06806527078151703, True Risk: 0.01067451573908329\n",
            "Pred Risk: 0.11050456017255783, True Risk: 0.01050837803632021\n",
            "Avg Train Excess Risk: 0.07138869166374207, Test Excess Risk: 0.0999961793422699\n",
            "Pred Risk: 0.07833278924226761, True Risk: 0.010065878741443157\n",
            "Pred Risk: 0.07994400709867477, True Risk: 0.009519281797111034\n",
            "Pred Risk: 0.06043902784585953, True Risk: 0.010288207791745663\n",
            "Pred Risk: 0.0738048180937767, True Risk: 0.009566523134708405\n",
            "Pred Risk: 0.06952501833438873, True Risk: 0.00999457947909832\n",
            "Pred Risk: 0.05780221149325371, True Risk: 0.01035205740481615\n",
            "Pred Risk: 0.06615766137838364, True Risk: 0.010395272634923458\n",
            "Pred Risk: 0.054048601537942886, True Risk: 0.009098107926547527\n",
            "Pred Risk: 0.06856358051300049, True Risk: 0.009909682907164097\n",
            "Pred Risk: 0.07980461418628693, True Risk: 0.00995644647628069\n",
            "Pred Risk: 0.1278712898492813, True Risk: 0.009630227461457253\n",
            "Avg Train Excess Risk: 0.05892762541770935, Test Excess Risk: 0.11824106425046921\n",
            "Pred Risk: 0.05075851082801819, True Risk: 0.010306566022336483\n",
            "Pred Risk: 0.0712575688958168, True Risk: 0.010011112317442894\n",
            "Pred Risk: 0.06749463826417923, True Risk: 0.009980550967156887\n",
            "Pred Risk: 0.06242307648062706, True Risk: 0.009931787848472595\n",
            "Pred Risk: 0.06876210123300552, True Risk: 0.009855575859546661\n",
            "Pred Risk: 0.06565733253955841, True Risk: 0.00992990005761385\n",
            "Pred Risk: 0.06456940621137619, True Risk: 0.00974286999553442\n",
            "Pred Risk: 0.06285379827022552, True Risk: 0.009964978322386742\n",
            "Pred Risk: 0.07536513358354568, True Risk: 0.010706296190619469\n",
            "Pred Risk: 0.0755501314997673, True Risk: 0.010578186251223087\n",
            "Pred Risk: 0.07564220577478409, True Risk: 0.009647178463637829\n",
            "Avg Train Excess Risk: 0.056368388235569, Test Excess Risk: 0.06599503010511398\n",
            "Pred Risk: 0.05817567929625511, True Risk: 0.010101416148245335\n",
            "Pred Risk: 0.07489357888698578, True Risk: 0.010367688722908497\n",
            "Pred Risk: 0.0919611006975174, True Risk: 0.00963569339364767\n",
            "Pred Risk: 0.06692884117364883, True Risk: 0.009873969480395317\n",
            "Pred Risk: 0.06798660755157471, True Risk: 0.009567337110638618\n",
            "Pred Risk: 0.06263688951730728, True Risk: 0.010517886839807034\n",
            "Pred Risk: 0.06176271289587021, True Risk: 0.010164023377001286\n",
            "Pred Risk: 0.06216498836874962, True Risk: 0.009658480994403362\n",
            "Pred Risk: 0.06838302314281464, True Risk: 0.01011470053344965\n",
            "Pred Risk: 0.07156376540660858, True Risk: 0.009715311229228973\n",
            "Pred Risk: 0.07904598116874695, True Risk: 0.009872306138277054\n",
            "Avg Train Excess Risk: 0.05867406725883484, Test Excess Risk: 0.06917367875576019\n",
            "Pred Risk: 0.053550612181425095, True Risk: 0.009638579562306404\n",
            "Pred Risk: 0.06516632437705994, True Risk: 0.00978472363203764\n",
            "Pred Risk: 0.06469932198524475, True Risk: 0.009455892257392406\n",
            "Pred Risk: 0.06723693013191223, True Risk: 0.010187377221882343\n",
            "Pred Risk: 0.06139837205410004, True Risk: 0.010304505936801434\n",
            "Pred Risk: 0.0750567764043808, True Risk: 0.009702770039439201\n",
            "Pred Risk: 0.08283913880586624, True Risk: 0.010625289753079414\n",
            "Pred Risk: 0.09400839358568192, True Risk: 0.009840946644544601\n",
            "Pred Risk: 0.06759623438119888, True Risk: 0.009236624464392662\n",
            "Pred Risk: 0.081539086997509, True Risk: 0.010891549289226532\n",
            "Pred Risk: 0.12407146394252777, True Risk: 0.00990995578467846\n",
            "Avg Train Excess Risk: 0.06134228780865669, Test Excess Risk: 0.11416150629520416\n",
            "Pred Risk: 0.052604567259550095, True Risk: 0.01032062154263258\n",
            "Pred Risk: 0.06487033516168594, True Risk: 0.010304681956768036\n",
            "Pred Risk: 0.058179233223199844, True Risk: 0.009976997040212154\n",
            "Pred Risk: 0.07379596680402756, True Risk: 0.009768759831786156\n",
            "Pred Risk: 0.08715501427650452, True Risk: 0.010353357531130314\n",
            "Pred Risk: 0.07415523380041122, True Risk: 0.010035621002316475\n",
            "Pred Risk: 0.05834878981113434, True Risk: 0.009836101904511452\n",
            "Pred Risk: 0.07113069295883179, True Risk: 0.010091844946146011\n",
            "Pred Risk: 0.07508646696805954, True Risk: 0.010330555960536003\n",
            "Pred Risk: 0.05834606662392616, True Risk: 0.010057894513010979\n",
            "Pred Risk: 0.09676133841276169, True Risk: 0.01015450432896614\n",
            "Avg Train Excess Risk: 0.057259589433670044, Test Excess Risk: 0.08660683035850525\n",
            "Pred Risk: 0.06877658516168594, True Risk: 0.009886123239994049\n",
            "Pred Risk: 0.07879283279180527, True Risk: 0.010353892110288143\n",
            "Pred Risk: 0.07779217511415482, True Risk: 0.010593574494123459\n",
            "Pred Risk: 0.07867217063903809, True Risk: 0.010439208708703518\n",
            "Pred Risk: 0.08147522807121277, True Risk: 0.010763902217149734\n",
            "Pred Risk: 0.09047798067331314, True Risk: 0.009710593149065971\n",
            "Pred Risk: 0.059905242174863815, True Risk: 0.009974983520805836\n",
            "Pred Risk: 0.0866493508219719, True Risk: 0.009678044356405735\n",
            "Pred Risk: 0.04976803809404373, True Risk: 0.009147313423454762\n",
            "Pred Risk: 0.07510733604431152, True Risk: 0.010749178938567638\n",
            "Pred Risk: 0.14088165760040283, True Risk: 0.01047978363931179\n",
            "Avg Train Excess Risk: 0.06461201608181, Test Excess Risk: 0.1304018795490265\n",
            "Pred Risk: 0.07961712032556534, True Risk: 0.009765557944774628\n",
            "Pred Risk: 0.06672849506139755, True Risk: 0.010033799335360527\n",
            "Pred Risk: 0.062252145260572433, True Risk: 0.00939150806516409\n",
            "Pred Risk: 0.10013140738010406, True Risk: 0.009773003868758678\n",
            "Pred Risk: 0.08854096382856369, True Risk: 0.00989793986082077\n",
            "Pred Risk: 0.0718085989356041, True Risk: 0.009696741588413715\n",
            "Pred Risk: 0.058946456760168076, True Risk: 0.010335218161344528\n",
            "Pred Risk: 0.06090950593352318, True Risk: 0.009939735755324364\n",
            "Pred Risk: 0.06764662265777588, True Risk: 0.010111064650118351\n",
            "Pred Risk: 0.05455048382282257, True Risk: 0.00962647795677185\n",
            "Pred Risk: 0.09064192324876785, True Risk: 0.00994700938463211\n",
            "Avg Train Excess Risk: 0.06125607341527939, Test Excess Risk: 0.08069491386413574\n",
            "Pred Risk: 0.061209481209516525, True Risk: 0.010193665511906147\n",
            "Pred Risk: 0.06310968101024628, True Risk: 0.010443688370287418\n",
            "Pred Risk: 0.06001642346382141, True Risk: 0.01052411925047636\n",
            "Pred Risk: 0.05861673504114151, True Risk: 0.01012854278087616\n",
            "Pred Risk: 0.05467342957854271, True Risk: 0.01023431308567524\n",
            "Pred Risk: 0.07181756943464279, True Risk: 0.010331539437174797\n",
            "Pred Risk: 0.04741433635354042, True Risk: 0.010027367621660233\n",
            "Pred Risk: 0.07616652548313141, True Risk: 0.010013636201620102\n",
            "Pred Risk: 0.0589393749833107, True Risk: 0.009712987579405308\n",
            "Pred Risk: 0.05917743593454361, True Risk: 0.010115298442542553\n",
            "Pred Risk: 0.09945322573184967, True Risk: 0.01009630598127842\n",
            "Avg Train Excess Risk: 0.050941579043865204, Test Excess Risk: 0.0893569216132164\n",
            "Pred Risk: 0.09251844137907028, True Risk: 0.010379878804087639\n",
            "Pred Risk: 0.0882515087723732, True Risk: 0.010196481831371784\n",
            "Pred Risk: 0.05284377187490463, True Risk: 0.009711968712508678\n",
            "Pred Risk: 0.0755477026104927, True Risk: 0.009940559044480324\n",
            "Pred Risk: 0.0693284273147583, True Risk: 0.010074738413095474\n",
            "Pred Risk: 0.05492544174194336, True Risk: 0.010236203670501709\n",
            "Pred Risk: 0.06384626030921936, True Risk: 0.010284928604960442\n",
            "Pred Risk: 0.08893028646707535, True Risk: 0.01016616728156805\n",
            "Pred Risk: 0.08869875222444534, True Risk: 0.009812013246119022\n",
            "Pred Risk: 0.08156770467758179, True Risk: 0.009351339191198349\n",
            "Pred Risk: 0.09564252942800522, True Risk: 0.010059138759970665\n",
            "Avg Train Excess Risk: 0.06563040614128113, Test Excess Risk: 0.0855833888053894\n",
            "Pred Risk: 0.058508630841970444, True Risk: 0.009977810084819794\n",
            "Pred Risk: 0.07063030451536179, True Risk: 0.009354565292596817\n",
            "Pred Risk: 0.08989708125591278, True Risk: 0.009850410744547844\n",
            "Pred Risk: 0.07056719064712524, True Risk: 0.010461880825459957\n",
            "Pred Risk: 0.06089753285050392, True Risk: 0.0097039844840765\n",
            "Pred Risk: 0.07938080281019211, True Risk: 0.009894453920423985\n",
            "Pred Risk: 0.0796140730381012, True Risk: 0.009779863059520721\n",
            "Pred Risk: 0.0668904259800911, True Risk: 0.009800886735320091\n",
            "Pred Risk: 0.08760696649551392, True Risk: 0.009910928085446358\n",
            "Pred Risk: 0.06642870604991913, True Risk: 0.010684236884117126\n",
            "Pred Risk: 0.12102139741182327, True Risk: 0.009600389748811722\n",
            "Avg Train Excess Risk: 0.06310027837753296, Test Excess Risk: 0.11142100393772125\n",
            "Pred Risk: 0.06688150018453598, True Risk: 0.010155403055250645\n",
            "Pred Risk: 0.07083006948232651, True Risk: 0.010353462770581245\n",
            "Pred Risk: 0.0773921087384224, True Risk: 0.01012895442545414\n",
            "Pred Risk: 0.0639229342341423, True Risk: 0.009987062774598598\n",
            "Pred Risk: 0.06490545719861984, True Risk: 0.010379776358604431\n",
            "Pred Risk: 0.06092798337340355, True Risk: 0.010117045603692532\n",
            "Pred Risk: 0.061826132237911224, True Risk: 0.009675377048552036\n",
            "Pred Risk: 0.041164398193359375, True Risk: 0.010254203341901302\n",
            "Pred Risk: 0.07318546622991562, True Risk: 0.009901936165988445\n",
            "Pred Risk: 0.06905653327703476, True Risk: 0.009766249917447567\n",
            "Pred Risk: 0.10357179492712021, True Risk: 0.010055937804281712\n",
            "Avg Train Excess Risk: 0.05493731051683426, Test Excess Risk: 0.09351585805416107\n",
            "Pred Risk: 0.06291192024946213, True Risk: 0.010409985668957233\n",
            "Pred Risk: 0.09093005210161209, True Risk: 0.010366310365498066\n",
            "Pred Risk: 0.07357583940029144, True Risk: 0.009901674464344978\n",
            "Pred Risk: 0.081450916826725, True Risk: 0.010042233392596245\n",
            "Pred Risk: 0.060712672770023346, True Risk: 0.010248499922454357\n",
            "Pred Risk: 0.048587366938591, True Risk: 0.009409104473888874\n",
            "Pred Risk: 0.06061333790421486, True Risk: 0.010757090523838997\n",
            "Pred Risk: 0.08429320901632309, True Risk: 0.009795498102903366\n",
            "Pred Risk: 0.06413114815950394, True Risk: 0.01000223122537136\n",
            "Pred Risk: 0.08891125023365021, True Risk: 0.009803962893784046\n",
            "Pred Risk: 0.12645268440246582, True Risk: 0.009852335788309574\n",
            "Avg Train Excess Risk: 0.06153811141848564, Test Excess Risk: 0.11660034954547882\n",
            "Pred Risk: 0.0654749646782875, True Risk: 0.008553155697882175\n",
            "Pred Risk: 0.054345786571502686, True Risk: 0.009708551689982414\n",
            "Pred Risk: 0.06705787032842636, True Risk: 0.00989287905395031\n",
            "Pred Risk: 0.0692795142531395, True Risk: 0.009683948010206223\n",
            "Pred Risk: 0.06106569990515709, True Risk: 0.01027972437441349\n",
            "Pred Risk: 0.0837135836482048, True Risk: 0.010009059682488441\n",
            "Pred Risk: 0.05742394179105759, True Risk: 0.010408680886030197\n",
            "Pred Risk: 0.05649224668741226, True Risk: 0.009898235090076923\n",
            "Pred Risk: 0.058101896196603775, True Risk: 0.009695058688521385\n",
            "Pred Risk: 0.03964711353182793, True Risk: 0.009422289207577705\n",
            "Pred Risk: 0.07810884714126587, True Risk: 0.009560888633131981\n",
            "Avg Train Excess Risk: 0.051505107432603836, Test Excess Risk: 0.06854795664548874\n",
            "Pred Risk: 0.059333864599466324, True Risk: 0.0099213607609272\n",
            "Pred Risk: 0.08603063225746155, True Risk: 0.010261597111821175\n",
            "Pred Risk: 0.071321040391922, True Risk: 0.009765556082129478\n",
            "Pred Risk: 0.05485955625772476, True Risk: 0.010115464217960835\n",
            "Pred Risk: 0.047143712639808655, True Risk: 0.010014732368290424\n",
            "Pred Risk: 0.074032261967659, True Risk: 0.009317907504737377\n",
            "Pred Risk: 0.06560011208057404, True Risk: 0.010010585188865662\n",
            "Pred Risk: 0.07354520261287689, True Risk: 0.009719761088490486\n",
            "Pred Risk: 0.05520753934979439, True Risk: 0.0099685313180089\n",
            "Pred Risk: 0.051224566996097565, True Risk: 0.010029043070971966\n",
            "Pred Risk: 0.10878877341747284, True Risk: 0.010126130655407906\n",
            "Avg Train Excess Risk: 0.05391739681363106, Test Excess Risk: 0.09866264462471008\n",
            "Pred Risk: 0.14229615032672882, True Risk: 0.010678565129637718\n",
            "Pred Risk: 0.05985523387789726, True Risk: 0.010016067884862423\n",
            "Pred Risk: 0.07219824194908142, True Risk: 0.009959041140973568\n",
            "Pred Risk: 0.059979189187288284, True Risk: 0.009808588773012161\n",
            "Pred Risk: 0.072597935795784, True Risk: 0.010264656506478786\n",
            "Pred Risk: 0.06061875820159912, True Risk: 0.009843965992331505\n",
            "Pred Risk: 0.08871441334486008, True Risk: 0.009378206916153431\n",
            "Pred Risk: 0.07045815885066986, True Risk: 0.010247375816106796\n",
            "Pred Risk: 0.06350601464509964, True Risk: 0.010373945347964764\n",
            "Pred Risk: 0.08114006370306015, True Risk: 0.01027554553002119\n",
            "Pred Risk: 0.11607073247432709, True Risk: 0.00980946235358715\n",
            "Avg Train Excess Risk: 0.06705182045698166, Test Excess Risk: 0.10626126825809479\n",
            "Pred Risk: 0.05816826969385147, True Risk: 0.010205371305346489\n",
            "Pred Risk: 0.06464165449142456, True Risk: 0.009767986834049225\n",
            "Pred Risk: 0.057800065726041794, True Risk: 0.009604956954717636\n",
            "Pred Risk: 0.053409475833177567, True Risk: 0.00977414008229971\n",
            "Pred Risk: 0.06471458822488785, True Risk: 0.009508439339697361\n",
            "Pred Risk: 0.06922967731952667, True Risk: 0.009855924174189568\n",
            "Pred Risk: 0.07449840009212494, True Risk: 0.01043578889220953\n",
            "Pred Risk: 0.07116538286209106, True Risk: 0.009339743293821812\n",
            "Pred Risk: 0.05820562317967415, True Risk: 0.010136234574019909\n",
            "Pred Risk: 0.07963616400957108, True Risk: 0.010886000469326973\n",
            "Pred Risk: 0.08620196580886841, True Risk: 0.010775248520076275\n",
            "Avg Train Excess Risk: 0.05519546940922737, Test Excess Risk: 0.07542672008275986\n",
            "Pred Risk: 0.07360733300447464, True Risk: 0.010507049039006233\n",
            "Pred Risk: 0.05499173328280449, True Risk: 0.010383985005319118\n",
            "Pred Risk: 0.06832636147737503, True Risk: 0.00980332586914301\n",
            "Pred Risk: 0.08145204186439514, True Risk: 0.009777214378118515\n",
            "Pred Risk: 0.06166151165962219, True Risk: 0.009602995589375496\n",
            "Pred Risk: 0.05838456004858017, True Risk: 0.009548800997436047\n",
            "Pred Risk: 0.08413361012935638, True Risk: 0.009614839218556881\n",
            "Pred Risk: 0.053810108453035355, True Risk: 0.010141279548406601\n",
            "Pred Risk: 0.053670793771743774, True Risk: 0.010312946513295174\n",
            "Pred Risk: 0.047878582030534744, True Risk: 0.009551377035677433\n",
            "Pred Risk: 0.11619697511196136, True Risk: 0.009633803740143776\n",
            "Avg Train Excess Risk: 0.05386728048324585, Test Excess Risk: 0.10656317323446274\n",
            "Pred Risk: 0.05086718127131462, True Risk: 0.01018537487834692\n",
            "Pred Risk: 0.07497788965702057, True Risk: 0.009904464706778526\n",
            "Pred Risk: 0.0677342638373375, True Risk: 0.009867646731436253\n",
            "Pred Risk: 0.09944582730531693, True Risk: 0.010192626155912876\n",
            "Pred Risk: 0.048519764095544815, True Risk: 0.010202622972428799\n",
            "Pred Risk: 0.06876399368047714, True Risk: 0.009999359026551247\n",
            "Pred Risk: 0.053718451410532, True Risk: 0.009681984782218933\n",
            "Pred Risk: 0.05561330169439316, True Risk: 0.010062582790851593\n",
            "Pred Risk: 0.05466420575976372, True Risk: 0.00995082687586546\n",
            "Pred Risk: 0.04877711459994316, True Risk: 0.009976980276405811\n",
            "Pred Risk: 0.07550187408924103, True Risk: 0.009637877345085144\n",
            "Avg Train Excess Risk: 0.05230575054883957, Test Excess Risk: 0.06586399674415588\n",
            "Pred Risk: 0.06662709265947342, True Risk: 0.010056144557893276\n",
            "Pred Risk: 0.06549523770809174, True Risk: 0.010327089577913284\n",
            "Pred Risk: 0.058553460985422134, True Risk: 0.009839776903390884\n",
            "Pred Risk: 0.06872385740280151, True Risk: 0.010374164208769798\n",
            "Pred Risk: 0.05818096175789833, True Risk: 0.009884494356811047\n",
            "Pred Risk: 0.07477510720491409, True Risk: 0.009312201291322708\n",
            "Pred Risk: 0.09278780966997147, True Risk: 0.00954029057174921\n",
            "Pred Risk: 0.05813372880220413, True Risk: 0.01095740869641304\n",
            "Pred Risk: 0.0614006407558918, True Risk: 0.010241727344691753\n",
            "Pred Risk: 0.06313047558069229, True Risk: 0.010222668759524822\n",
            "Pred Risk: 0.09560851007699966, True Risk: 0.009612048976123333\n",
            "Avg Train Excess Risk: 0.05670524388551712, Test Excess Risk: 0.08599646389484406\n",
            "Pred Risk: 0.07286939024925232, True Risk: 0.010220592841506004\n",
            "Pred Risk: 0.06534014642238617, True Risk: 0.010273301042616367\n",
            "Pred Risk: 0.04622460901737213, True Risk: 0.0098176971077919\n",
            "Pred Risk: 0.049912404268980026, True Risk: 0.009694923646748066\n",
            "Pred Risk: 0.05159517750144005, True Risk: 0.011435582302510738\n",
            "Pred Risk: 0.04736640304327011, True Risk: 0.010240829549729824\n",
            "Pred Risk: 0.06767825782299042, True Risk: 0.010693741030991077\n",
            "Pred Risk: 0.07092529535293579, True Risk: 0.010363725945353508\n",
            "Pred Risk: 0.05999040603637695, True Risk: 0.010000782087445259\n",
            "Pred Risk: 0.04610316455364227, True Risk: 0.009934264235198498\n",
            "Pred Risk: 0.07723647356033325, True Risk: 0.010165889747440815\n",
            "Avg Train Excess Risk: 0.047532983124256134, Test Excess Risk: 0.06707058101892471\n",
            "Pred Risk: 0.06152767688035965, True Risk: 0.010516910813748837\n",
            "Pred Risk: 0.08304116874933243, True Risk: 0.009854854084551334\n",
            "Pred Risk: 0.07418743520975113, True Risk: 0.010409364476799965\n",
            "Pred Risk: 0.09211070090532303, True Risk: 0.009635945782065392\n",
            "Pred Risk: 0.0819234624505043, True Risk: 0.010121586732566357\n",
            "Pred Risk: 0.07388229668140411, True Risk: 0.009653297252953053\n",
            "Pred Risk: 0.05356866121292114, True Risk: 0.010234098881483078\n",
            "Pred Risk: 0.07600054144859314, True Risk: 0.010679318569600582\n",
            "Pred Risk: 0.07818035781383514, True Risk: 0.00967315025627613\n",
            "Pred Risk: 0.06806683540344238, True Risk: 0.01031938660889864\n",
            "Pred Risk: 0.11934983730316162, True Risk: 0.010243848897516727\n",
            "Avg Train Excess Risk: 0.06413911283016205, Test Excess Risk: 0.10910598933696747\n",
            "Pred Risk: 0.06991893798112869, True Risk: 0.010389452800154686\n",
            "Pred Risk: 0.06687679886817932, True Risk: 0.00984489917755127\n",
            "Pred Risk: 0.07417332381010056, True Risk: 0.010187375359237194\n",
            "Pred Risk: 0.07873524725437164, True Risk: 0.009920851327478886\n",
            "Pred Risk: 0.05940033495426178, True Risk: 0.010051162913441658\n",
            "Pred Risk: 0.092646025121212, True Risk: 0.009916499257087708\n",
            "Pred Risk: 0.052275992929935455, True Risk: 0.009946067817509174\n",
            "Pred Risk: 0.10873395204544067, True Risk: 0.010400311090052128\n",
            "Pred Risk: 0.07317168265581131, True Risk: 0.009653407149016857\n",
            "Pred Risk: 0.05086427927017212, True Risk: 0.009879214689135551\n",
            "Pred Risk: 0.08720483630895615, True Risk: 0.00974390096962452\n",
            "Avg Train Excess Risk: 0.06266073137521744, Test Excess Risk: 0.07746093720197678\n",
            "Pred Risk: 0.056572310626506805, True Risk: 0.009787049144506454\n",
            "Pred Risk: 0.062149468809366226, True Risk: 0.010766961611807346\n",
            "Pred Risk: 0.08062295615673065, True Risk: 0.010008840821683407\n",
            "Pred Risk: 0.06744494289159775, True Risk: 0.0097816102206707\n",
            "Pred Risk: 0.08590088784694672, True Risk: 0.009369234554469585\n",
            "Pred Risk: 0.07040859758853912, True Risk: 0.010479697026312351\n",
            "Pred Risk: 0.07347177714109421, True Risk: 0.010481292381882668\n",
            "Pred Risk: 0.042951878160238266, True Risk: 0.009989789687097073\n",
            "Pred Risk: 0.058146439492702484, True Risk: 0.009704969823360443\n",
            "Pred Risk: 0.08047077804803848, True Risk: 0.010448460467159748\n",
            "Pred Risk: 0.06710401177406311, True Risk: 0.010159488767385483\n",
            "Avg Train Excess Risk: 0.05773221328854561, Test Excess Risk: 0.05694452300667763\n",
            "Pred Risk: 0.08570042997598648, True Risk: 0.009478949941694736\n",
            "Pred Risk: 0.0725197121500969, True Risk: 0.00987168401479721\n",
            "Pred Risk: 0.06332867592573166, True Risk: 0.009413143619894981\n",
            "Pred Risk: 0.07796642184257507, True Risk: 0.010085818357765675\n",
            "Pred Risk: 0.07680735737085342, True Risk: 0.010197494179010391\n",
            "Pred Risk: 0.08288268744945526, True Risk: 0.010456092655658722\n",
            "Pred Risk: 0.08759491890668869, True Risk: 0.010469521395862103\n",
            "Pred Risk: 0.06499933451414108, True Risk: 0.009943907149136066\n",
            "Pred Risk: 0.07160713523626328, True Risk: 0.00967912096530199\n",
            "Pred Risk: 0.08339208364486694, True Risk: 0.009760582819581032\n",
            "Pred Risk: 0.11144384741783142, True Risk: 0.009682253934442997\n",
            "Avg Train Excess Risk: 0.06674425303936005, Test Excess Risk: 0.101761594414711\n",
            "Pred Risk: 0.05962224304676056, True Risk: 0.011018039658665657\n",
            "Pred Risk: 0.05974002182483673, True Risk: 0.009657085873186588\n",
            "Pred Risk: 0.05810194090008736, True Risk: 0.010142411105334759\n",
            "Pred Risk: 0.0765257403254509, True Risk: 0.009736239910125732\n",
            "Pred Risk: 0.07875698804855347, True Risk: 0.009943623095750809\n",
            "Pred Risk: 0.06928553432226181, True Risk: 0.01039663702249527\n",
            "Pred Risk: 0.062961146235466, True Risk: 0.010227675549685955\n",
            "Pred Risk: 0.07840847223997116, True Risk: 0.010299819521605968\n",
            "Pred Risk: 0.07516555488109589, True Risk: 0.009737534448504448\n",
            "Pred Risk: 0.05321859195828438, True Risk: 0.01034857053309679\n",
            "Pred Risk: 0.11169910430908203, True Risk: 0.01009867899119854\n",
            "Avg Train Excess Risk: 0.05702786520123482, Test Excess Risk: 0.10160042345523834\n",
            "Pred Risk: 0.061260756105184555, True Risk: 0.009824096225202084\n",
            "Pred Risk: 0.05440949648618698, True Risk: 0.009892772883176804\n",
            "Pred Risk: 0.05721481889486313, True Risk: 0.009370347484946251\n",
            "Pred Risk: 0.04710642993450165, True Risk: 0.010967007838189602\n",
            "Pred Risk: 0.06718519330024719, True Risk: 0.009762647561728954\n",
            "Pred Risk: 0.06654074788093567, True Risk: 0.009901748970150948\n",
            "Pred Risk: 0.06254991888999939, True Risk: 0.009937243536114693\n",
            "Pred Risk: 0.07925022393465042, True Risk: 0.010252685286104679\n",
            "Pred Risk: 0.07511545717716217, True Risk: 0.01005308236926794\n",
            "Pred Risk: 0.0712476596236229, True Risk: 0.009567494504153728\n",
            "Pred Risk: 0.08476562798023224, True Risk: 0.01003418117761612\n",
            "Avg Train Excess Risk: 0.05423516035079956, Test Excess Risk: 0.07473144680261612\n",
            "Pred Risk: 0.08303609490394592, True Risk: 0.009311194531619549\n",
            "Pred Risk: 0.09701930731534958, True Risk: 0.009108916856348515\n",
            "Pred Risk: 0.11469025909900665, True Risk: 0.010454488918185234\n",
            "Pred Risk: 0.07501323521137238, True Risk: 0.010242734104394913\n",
            "Pred Risk: 0.10462960600852966, True Risk: 0.00963690783828497\n",
            "Pred Risk: 0.08514272421598434, True Risk: 0.009563407860696316\n",
            "Pred Risk: 0.07391662150621414, True Risk: 0.010175241157412529\n",
            "Pred Risk: 0.09721972048282623, True Risk: 0.009707562625408173\n",
            "Pred Risk: 0.12413312494754791, True Risk: 0.009876620955765247\n",
            "Pred Risk: 0.08283927291631699, True Risk: 0.010139941237866879\n",
            "Pred Risk: 0.1217740997672081, True Risk: 0.010015226900577545\n",
            "Avg Train Excess Risk: 0.08394228667020798, Test Excess Risk: 0.11175887286663055\n",
            "Pred Risk: 0.04339507594704628, True Risk: 0.01031701173633337\n",
            "Pred Risk: 0.05828968062996864, True Risk: 0.010423458181321621\n",
            "Pred Risk: 0.07235205918550491, True Risk: 0.010229640640318394\n",
            "Pred Risk: 0.06420287489891052, True Risk: 0.010191311128437519\n",
            "Pred Risk: 0.07422926276922226, True Risk: 0.008950697258114815\n",
            "Pred Risk: 0.07627571374177933, True Risk: 0.009803918190300465\n",
            "Pred Risk: 0.07523384690284729, True Risk: 0.009961632080376148\n",
            "Pred Risk: 0.07907193899154663, True Risk: 0.009649625979363918\n",
            "Pred Risk: 0.05433112382888794, True Risk: 0.010629571042954922\n",
            "Pred Risk: 0.07751017808914185, True Risk: 0.009785865433514118\n",
            "Pred Risk: 0.10385571420192719, True Risk: 0.010019093751907349\n",
            "Avg Train Excess Risk: 0.05749489739537239, Test Excess Risk: 0.09383662045001984\n",
            "Pred Risk: 0.07912910729646683, True Risk: 0.00991304311901331\n",
            "Pred Risk: 0.07121672481298447, True Risk: 0.010843358002603054\n",
            "Pred Risk: 0.06669171154499054, True Risk: 0.010038108564913273\n",
            "Pred Risk: 0.09732755273580551, True Risk: 0.010498080402612686\n",
            "Pred Risk: 0.052696190774440765, True Risk: 0.009623797610402107\n",
            "Pred Risk: 0.06713967025279999, True Risk: 0.010038679465651512\n",
            "Pred Risk: 0.07899443805217743, True Risk: 0.009908242151141167\n",
            "Pred Risk: 0.06201307475566864, True Risk: 0.00942468736320734\n",
            "Pred Risk: 0.06030731275677681, True Risk: 0.009633464738726616\n",
            "Pred Risk: 0.05324750393629074, True Risk: 0.009701485745608807\n",
            "Pred Risk: 0.10260012745857239, True Risk: 0.00973506923764944\n",
            "Avg Train Excess Risk: 0.05891403555870056, Test Excess Risk: 0.09286505728960037\n",
            "Pred Risk: 0.04417116194963455, True Risk: 0.009872517548501492\n",
            "Pred Risk: 0.08385664224624634, True Risk: 0.009899471886456013\n",
            "Pred Risk: 0.06033605709671974, True Risk: 0.009873056784272194\n",
            "Pred Risk: 0.05541038513183594, True Risk: 0.010485900565981865\n",
            "Pred Risk: 0.06204661726951599, True Risk: 0.009998250752687454\n",
            "Pred Risk: 0.06778288632631302, True Risk: 0.009657658636569977\n",
            "Pred Risk: 0.08153868466615677, True Risk: 0.009724574163556099\n",
            "Pred Risk: 0.05739777162671089, True Risk: 0.009529011324048042\n",
            "Pred Risk: 0.05660474672913551, True Risk: 0.009365350939333439\n",
            "Pred Risk: 0.06262708455324173, True Risk: 0.01038585975766182\n",
            "Pred Risk: 0.07955002784729004, True Risk: 0.009421154856681824\n",
            "Avg Train Excess Risk: 0.05329803377389908, Test Excess Risk: 0.07012887299060822\n",
            "Pred Risk: 0.06797615438699722, True Risk: 0.010530074127018452\n",
            "Pred Risk: 0.07393446564674377, True Risk: 0.00958486832678318\n",
            "Pred Risk: 0.08592169731855392, True Risk: 0.010107123292982578\n",
            "Pred Risk: 0.11373474448919296, True Risk: 0.010660680010914803\n",
            "Pred Risk: 0.12838894128799438, True Risk: 0.00936555489897728\n",
            "Pred Risk: 0.07521040737628937, True Risk: 0.01012885756790638\n",
            "Pred Risk: 0.08881830424070358, True Risk: 0.009384909644722939\n",
            "Pred Risk: 0.07338248193264008, True Risk: 0.009399131871759892\n",
            "Pred Risk: 0.10482478886842728, True Risk: 0.009663526900112629\n",
            "Pred Risk: 0.07663523405790329, True Risk: 0.010073629207909107\n",
            "Pred Risk: 0.1437685638666153, True Risk: 0.010168911889195442\n",
            "Avg Train Excess Risk: 0.07899288088083267, Test Excess Risk: 0.133599653840065\n",
            "Pred Risk: 0.0713803693652153, True Risk: 0.010104961693286896\n",
            "Pred Risk: 0.05037339776754379, True Risk: 0.009931749664247036\n",
            "Pred Risk: 0.0676792785525322, True Risk: 0.010662774555385113\n",
            "Pred Risk: 0.06861424446105957, True Risk: 0.010016661137342453\n",
            "Pred Risk: 0.06060086563229561, True Risk: 0.010526006110012531\n",
            "Pred Risk: 0.08335352689027786, True Risk: 0.009375844150781631\n",
            "Pred Risk: 0.07890477776527405, True Risk: 0.01012366358190775\n",
            "Pred Risk: 0.07216877490282059, True Risk: 0.010071749798953533\n",
            "Pred Risk: 0.06849073618650436, True Risk: 0.009683478623628616\n",
            "Pred Risk: 0.07528083771467209, True Risk: 0.009699291549623013\n",
            "Pred Risk: 0.08501666784286499, True Risk: 0.00982962641865015\n",
            "Avg Train Excess Risk: 0.05966506153345108, Test Excess Risk: 0.07518704235553741\n",
            "Pred Risk: 0.06656980514526367, True Risk: 0.009535926394164562\n",
            "Pred Risk: 0.0773661881685257, True Risk: 0.009551354683935642\n",
            "Pred Risk: 0.06477784365415573, True Risk: 0.010036900639533997\n",
            "Pred Risk: 0.07582803070545197, True Risk: 0.010205059312283993\n",
            "Pred Risk: 0.06417223811149597, True Risk: 0.009912766516208649\n",
            "Pred Risk: 0.06331374496221542, True Risk: 0.010488741099834442\n",
            "Pred Risk: 0.06184932589530945, True Risk: 0.009756658226251602\n",
            "Pred Risk: 0.062356822192668915, True Risk: 0.010110015980899334\n",
            "Pred Risk: 0.06929239630699158, True Risk: 0.009989133104681969\n",
            "Pred Risk: 0.0798950344324112, True Risk: 0.009724640287458897\n",
            "Pred Risk: 0.08333858102560043, True Risk: 0.010059361346065998\n",
            "Avg Train Excess Risk: 0.05861102417111397, Test Excess Risk: 0.07327921688556671\n",
            "Pred Risk: 0.060029856860637665, True Risk: 0.009780552238225937\n",
            "Pred Risk: 0.07266459614038467, True Risk: 0.010220359079539776\n",
            "Pred Risk: 0.08757148683071136, True Risk: 0.010523932985961437\n",
            "Pred Risk: 0.0753188282251358, True Risk: 0.010350596159696579\n",
            "Pred Risk: 0.061954498291015625, True Risk: 0.009699918329715729\n",
            "Pred Risk: 0.0683465376496315, True Risk: 0.010377723723649979\n",
            "Pred Risk: 0.05925571545958519, True Risk: 0.010110514238476753\n",
            "Pred Risk: 0.05001158267259598, True Risk: 0.011037406511604786\n",
            "Pred Risk: 0.055226098746061325, True Risk: 0.010093762539327145\n",
            "Pred Risk: 0.0628608763217926, True Risk: 0.009818561375141144\n",
            "Pred Risk: 0.10671717673540115, True Risk: 0.010022607631981373\n",
            "Avg Train Excess Risk: 0.055122680962085724, Test Excess Risk: 0.09669456630945206\n",
            "Pred Risk: 0.05688374862074852, True Risk: 0.010400821454823017\n",
            "Pred Risk: 0.05323398858308792, True Risk: 0.010334501042962074\n",
            "Pred Risk: 0.06448905169963837, True Risk: 0.009653543122112751\n",
            "Pred Risk: 0.05887579545378685, True Risk: 0.010104911401867867\n",
            "Pred Risk: 0.0528867170214653, True Risk: 0.009948906488716602\n",
            "Pred Risk: 0.05182702839374542, True Risk: 0.009989342652261257\n",
            "Pred Risk: 0.06112203001976013, True Risk: 0.010514461435377598\n",
            "Pred Risk: 0.052045948803424835, True Risk: 0.009558049030601978\n",
            "Pred Risk: 0.08838798105716705, True Risk: 0.010392571799457073\n",
            "Pred Risk: 0.0693749263882637, True Risk: 0.009596738032996655\n",
            "Pred Risk: 0.09486429393291473, True Risk: 0.01019810140132904\n",
            "Avg Train Excess Risk: 0.05086333677172661, Test Excess Risk: 0.0846661925315857\n",
            "Pred Risk: 0.053580958396196365, True Risk: 0.01027325913310051\n",
            "Pred Risk: 0.051851361989974976, True Risk: 0.010313876904547215\n",
            "Pred Risk: 0.06459768116474152, True Risk: 0.009256518445909023\n",
            "Pred Risk: 0.06422553211450577, True Risk: 0.010809686034917831\n",
            "Pred Risk: 0.06394579261541367, True Risk: 0.00977232214063406\n",
            "Pred Risk: 0.045318249613046646, True Risk: 0.010371509939432144\n",
            "Pred Risk: 0.06938302516937256, True Risk: 0.010246656835079193\n",
            "Pred Risk: 0.05387002229690552, True Risk: 0.00982331857085228\n",
            "Pred Risk: 0.06179779767990112, True Risk: 0.009410740807652473\n",
            "Pred Risk: 0.05413820222020149, True Risk: 0.009813600219786167\n",
            "Pred Risk: 0.07722710072994232, True Risk: 0.009918599389493465\n",
            "Avg Train Excess Risk: 0.04826170951128006, Test Excess Risk: 0.06730850040912628\n",
            "Pred Risk: 0.04474427178502083, True Risk: 0.00980051513761282\n",
            "Pred Risk: 0.05572979152202606, True Risk: 0.00978203397244215\n",
            "Pred Risk: 0.05218951404094696, True Risk: 0.010375604964792728\n",
            "Pred Risk: 0.06516119837760925, True Risk: 0.010119959712028503\n",
            "Pred Risk: 0.05592242628335953, True Risk: 0.009582440368831158\n",
            "Pred Risk: 0.05023987591266632, True Risk: 0.009944714605808258\n",
            "Pred Risk: 0.0455692782998085, True Risk: 0.009563851170241833\n",
            "Pred Risk: 0.052488379180431366, True Risk: 0.009809836745262146\n",
            "Pred Risk: 0.04964246600866318, True Risk: 0.00972251407802105\n",
            "Pred Risk: 0.064327172935009, True Risk: 0.01031456608325243\n",
            "Pred Risk: 0.07489010691642761, True Risk: 0.010516282171010971\n",
            "Avg Train Excess Risk: 0.04369983449578285, Test Excess Risk: 0.06437382102012634\n",
            "Pred Risk: 0.05180858448147774, True Risk: 0.010111783631145954\n",
            "Pred Risk: 0.04963751137256622, True Risk: 0.010363439098000526\n",
            "Pred Risk: 0.06189886853098869, True Risk: 0.010112296789884567\n",
            "Pred Risk: 0.05071492865681648, True Risk: 0.010278967209160328\n",
            "Pred Risk: 0.06536995619535446, True Risk: 0.010255509056150913\n",
            "Pred Risk: 0.05669048801064491, True Risk: 0.010838217101991177\n",
            "Pred Risk: 0.047499604523181915, True Risk: 0.010023456066846848\n",
            "Pred Risk: 0.045337654650211334, True Risk: 0.010127976536750793\n",
            "Pred Risk: 0.07754340767860413, True Risk: 0.009748485870659351\n",
            "Pred Risk: 0.06252317130565643, True Risk: 0.009657408110797405\n",
            "Pred Risk: 0.06951428204774857, True Risk: 0.009950440376996994\n",
            "Avg Train Excess Risk: 0.046750664710998535, Test Excess Risk: 0.05956384167075157\n",
            "Pred Risk: 0.03497267886996269, True Risk: 0.009998583234846592\n",
            "Pred Risk: 0.06078730896115303, True Risk: 0.009665928781032562\n",
            "Pred Risk: 0.054181721061468124, True Risk: 0.01029477920383215\n",
            "Pred Risk: 0.05325545370578766, True Risk: 0.01038558129221201\n",
            "Pred Risk: 0.045671869069337845, True Risk: 0.010154655203223228\n",
            "Pred Risk: 0.08105509728193283, True Risk: 0.010805296711623669\n",
            "Pred Risk: 0.04924684762954712, True Risk: 0.009676648303866386\n",
            "Pred Risk: 0.06014109402894974, True Risk: 0.009370417334139347\n",
            "Pred Risk: 0.03387720510363579, True Risk: 0.010071122087538242\n",
            "Pred Risk: 0.07483421266078949, True Risk: 0.009710338898003101\n",
            "Pred Risk: 0.0907316729426384, True Risk: 0.010328530333936214\n",
            "Avg Train Excess Risk: 0.044789012521505356, Test Excess Risk: 0.08040314167737961\n",
            "Pred Risk: 0.09682118892669678, True Risk: 0.010419161058962345\n",
            "Pred Risk: 0.07286853343248367, True Risk: 0.010119026526808739\n",
            "Pred Risk: 0.06584188342094421, True Risk: 0.010593612678349018\n",
            "Pred Risk: 0.07346232235431671, True Risk: 0.010074998252093792\n",
            "Pred Risk: 0.08009365200996399, True Risk: 0.010269152000546455\n",
            "Pred Risk: 0.05982367321848869, True Risk: 0.010371411219239235\n",
            "Pred Risk: 0.07390651851892471, True Risk: 0.01022915355861187\n",
            "Pred Risk: 0.05960679426789284, True Risk: 0.0099199078977108\n",
            "Pred Risk: 0.06862370669841766, True Risk: 0.009799974039196968\n",
            "Pred Risk: 0.06283585727214813, True Risk: 0.01001820433884859\n",
            "Pred Risk: 0.07573702931404114, True Risk: 0.009508544579148293\n",
            "Avg Train Excess Risk: 0.06120695546269417, Test Excess Risk: 0.066228486597538\n",
            "Pred Risk: 0.06926118582487106, True Risk: 0.010918324813246727\n",
            "Pred Risk: 0.0764164999127388, True Risk: 0.010165919549763203\n",
            "Pred Risk: 0.06628035008907318, True Risk: 0.010378076694905758\n",
            "Pred Risk: 0.059996433556079865, True Risk: 0.01011385302990675\n",
            "Pred Risk: 0.053039830178022385, True Risk: 0.00966207217425108\n",
            "Pred Risk: 0.058436546474695206, True Risk: 0.010038192383944988\n",
            "Pred Risk: 0.06717511266469955, True Risk: 0.009718121960759163\n",
            "Pred Risk: 0.06386282294988632, True Risk: 0.010451719164848328\n",
            "Pred Risk: 0.05724107846617699, True Risk: 0.009848677553236485\n",
            "Pred Risk: 0.058710671961307526, True Risk: 0.010125139728188515\n",
            "Pred Risk: 0.10293407738208771, True Risk: 0.010428146459162235\n",
            "Avg Train Excess Risk: 0.0529000461101532, Test Excess Risk: 0.09250593185424805\n",
            "Pred Risk: 0.05782177671790123, True Risk: 0.009986749850213528\n",
            "Pred Risk: 0.06352493166923523, True Risk: 0.009782249107956886\n",
            "Pred Risk: 0.061710573732852936, True Risk: 0.00975228101015091\n",
            "Pred Risk: 0.06188235059380531, True Risk: 0.010325557552278042\n",
            "Pred Risk: 0.07101723551750183, True Risk: 0.009788209572434425\n",
            "Pred Risk: 0.07383798807859421, True Risk: 0.009864059276878834\n",
            "Pred Risk: 0.055194396525621414, True Risk: 0.009944011457264423\n",
            "Pred Risk: 0.08701349794864655, True Risk: 0.009920063428580761\n",
            "Pred Risk: 0.05915355682373047, True Risk: 0.010527212172746658\n",
            "Pred Risk: 0.054798658937215805, True Risk: 0.009859766811132431\n",
            "Pred Risk: 0.0890674963593483, True Risk: 0.009566537104547024\n",
            "Avg Train Excess Risk: 0.05462048202753067, Test Excess Risk: 0.0795009583234787\n",
            "Pred Risk: 0.06350431591272354, True Risk: 0.010337798856198788\n",
            "Pred Risk: 0.060477644205093384, True Risk: 0.009974148124456406\n",
            "Pred Risk: 0.05389745160937309, True Risk: 0.009708330035209656\n",
            "Pred Risk: 0.05211115628480911, True Risk: 0.009376909583806992\n",
            "Pred Risk: 0.0471479594707489, True Risk: 0.009494610130786896\n",
            "Pred Risk: 0.06132284179329872, True Risk: 0.010191197507083416\n",
            "Pred Risk: 0.08329246193170547, True Risk: 0.009697526693344116\n",
            "Pred Risk: 0.04991571605205536, True Risk: 0.00950714386999607\n",
            "Pred Risk: 0.05251285806298256, True Risk: 0.01063215360045433\n",
            "Pred Risk: 0.06524480134248734, True Risk: 0.01047533005475998\n",
            "Pred Risk: 0.07290221005678177, True Risk: 0.009285581298172474\n",
            "Avg Train Excess Risk: 0.04900320619344711, Test Excess Risk: 0.06361662596464157\n",
            "Pred Risk: 0.04314620420336723, True Risk: 0.010266224853694439\n",
            "Pred Risk: 0.04114656522870064, True Risk: 0.009852354414761066\n",
            "Pred Risk: 0.04388804733753204, True Risk: 0.010206042788922787\n",
            "Pred Risk: 0.03774319216609001, True Risk: 0.009643271565437317\n",
            "Pred Risk: 0.046194229274988174, True Risk: 0.010657605715095997\n",
            "Pred Risk: 0.044983990490436554, True Risk: 0.010277122259140015\n",
            "Pred Risk: 0.06387581676244736, True Risk: 0.010586380027234554\n",
            "Pred Risk: 0.046541728079319, True Risk: 0.009769825264811516\n",
            "Pred Risk: 0.05096718296408653, True Risk: 0.010184573009610176\n",
            "Pred Risk: 0.05305655300617218, True Risk: 0.010072514414787292\n",
            "Pred Risk: 0.0728677362203598, True Risk: 0.009400119073688984\n",
            "Avg Train Excess Risk: 0.03700276091694832, Test Excess Risk: 0.0634676143527031\n",
            "Pred Risk: 0.07523464411497116, True Risk: 0.009775315411388874\n",
            "Pred Risk: 0.07325157523155212, True Risk: 0.010385286062955856\n",
            "Pred Risk: 0.06828323751688004, True Risk: 0.010048769414424896\n",
            "Pred Risk: 0.0860225260257721, True Risk: 0.009294732473790646\n",
            "Pred Risk: 0.04938492923974991, True Risk: 0.009784194640815258\n",
            "Pred Risk: 0.05818680301308632, True Risk: 0.010163971222937107\n",
            "Pred Risk: 0.06609901785850525, True Risk: 0.010259479284286499\n",
            "Pred Risk: 0.06596779823303223, True Risk: 0.010107518173754215\n",
            "Pred Risk: 0.06515076011419296, True Risk: 0.01054264698177576\n",
            "Pred Risk: 0.10357489436864853, True Risk: 0.010030136443674564\n",
            "Pred Risk: 0.15163935720920563, True Risk: 0.01034310832619667\n",
            "Avg Train Excess Risk: 0.061076413840055466, Test Excess Risk: 0.14129625260829926\n",
            "Pred Risk: 0.05337982997298241, True Risk: 0.01034049317240715\n",
            "Pred Risk: 0.06045721843838692, True Risk: 0.009989842772483826\n",
            "Pred Risk: 0.06966497004032135, True Risk: 0.00996378529816866\n",
            "Pred Risk: 0.049246884882450104, True Risk: 0.010238728486001492\n",
            "Pred Risk: 0.05431095138192177, True Risk: 0.009621739387512207\n",
            "Pred Risk: 0.03973889350891113, True Risk: 0.009485981427133083\n",
            "Pred Risk: 0.05297238752245903, True Risk: 0.009923558682203293\n",
            "Pred Risk: 0.058011170476675034, True Risk: 0.00992403831332922\n",
            "Pred Risk: 0.038971565663814545, True Risk: 0.00999535247683525\n",
            "Pred Risk: 0.06217062100768089, True Risk: 0.010249585844576359\n",
            "Pred Risk: 0.08945267647504807, True Risk: 0.009547901339828968\n",
            "Avg Train Excess Risk: 0.04391913861036301, Test Excess Risk: 0.07990477234125137\n",
            "Pred Risk: 0.07905056327581406, True Risk: 0.009746285155415535\n",
            "Pred Risk: 0.08601999282836914, True Risk: 0.009329712949693203\n",
            "Pred Risk: 0.05221089720726013, True Risk: 0.009700293652713299\n",
            "Pred Risk: 0.06783609092235565, True Risk: 0.010529386810958385\n",
            "Pred Risk: 0.07197005301713943, True Risk: 0.009656344540417194\n",
            "Pred Risk: 0.07926750928163528, True Risk: 0.01011697482317686\n",
            "Pred Risk: 0.055338867008686066, True Risk: 0.010366236791014671\n",
            "Pred Risk: 0.06848503649234772, True Risk: 0.008900651708245277\n",
            "Pred Risk: 0.06199405714869499, True Risk: 0.01017466839402914\n",
            "Pred Risk: 0.04752591624855995, True Risk: 0.010784545913338661\n",
            "Pred Risk: 0.10743574798107147, True Risk: 0.010217945091426373\n",
            "Avg Train Excess Risk: 0.05703939124941826, Test Excess Risk: 0.09721780568361282\n",
            "Pred Risk: 0.07414370775222778, True Risk: 0.01037830114364624\n",
            "Pred Risk: 0.07325321435928345, True Risk: 0.009788419120013714\n",
            "Pred Risk: 0.08075661957263947, True Risk: 0.010107997804880142\n",
            "Pred Risk: 0.07421967387199402, True Risk: 0.010061920620501041\n",
            "Pred Risk: 0.05584339052438736, True Risk: 0.010007512755692005\n",
            "Pred Risk: 0.059108104556798935, True Risk: 0.010065068490803242\n",
            "Pred Risk: 0.07721089571714401, True Risk: 0.010559961199760437\n",
            "Pred Risk: 0.08285267651081085, True Risk: 0.009521582163870335\n",
            "Pred Risk: 0.09518348425626755, True Risk: 0.01028379611670971\n",
            "Pred Risk: 0.09602836519479752, True Risk: 0.009504489600658417\n",
            "Pred Risk: 0.14444850385189056, True Risk: 0.00976578425616026\n",
            "Avg Train Excess Risk: 0.06683211028575897, Test Excess Risk: 0.13468271493911743\n",
            "Pred Risk: 0.07180283963680267, True Risk: 0.010360172018408775\n",
            "Pred Risk: 0.07241403311491013, True Risk: 0.009753702208399773\n",
            "Pred Risk: 0.07681982964277267, True Risk: 0.009656931273639202\n",
            "Pred Risk: 0.06289082020521164, True Risk: 0.010628734715282917\n",
            "Pred Risk: 0.06363813579082489, True Risk: 0.009881645441055298\n",
            "Pred Risk: 0.057269975543022156, True Risk: 0.01092114020138979\n",
            "Pred Risk: 0.06888221204280853, True Risk: 0.009640468284487724\n",
            "Pred Risk: 0.07593691349029541, True Risk: 0.010280809365212917\n",
            "Pred Risk: 0.0709257647395134, True Risk: 0.010166671127080917\n",
            "Pred Risk: 0.07819167524576187, True Risk: 0.010293252766132355\n",
            "Pred Risk: 0.10148952156305313, True Risk: 0.010082916356623173\n",
            "Avg Train Excess Risk: 0.059718869626522064, Test Excess Risk: 0.09140660613775253\n",
            "Pred Risk: 0.08695743978023529, True Risk: 0.01071994286030531\n",
            "Pred Risk: 0.06797400116920471, True Risk: 0.010056148283183575\n",
            "Pred Risk: 0.06874273717403412, True Risk: 0.009785404428839684\n",
            "Pred Risk: 0.054329764097929, True Risk: 0.00958266668021679\n",
            "Pred Risk: 0.06282752007246017, True Risk: 0.010055642575025558\n",
            "Pred Risk: 0.05124315619468689, True Risk: 0.010468544438481331\n",
            "Pred Risk: 0.07476238906383514, True Risk: 0.010377602651715279\n",
            "Pred Risk: 0.0485265888273716, True Risk: 0.010316384956240654\n",
            "Pred Risk: 0.0742555782198906, True Risk: 0.009540917351841927\n",
            "Pred Risk: 0.07314042001962662, True Risk: 0.009929060935974121\n",
            "Pred Risk: 0.08989422768354416, True Risk: 0.009826548397541046\n",
            "Avg Train Excess Risk: 0.056192733347415924, Test Excess Risk: 0.08006767928600311\n",
            "Pred Risk: 0.06353398412466049, True Risk: 0.010611425153911114\n",
            "Pred Risk: 0.0716201663017273, True Risk: 0.009592981077730656\n",
            "Pred Risk: 0.09244604408740997, True Risk: 0.01019766740500927\n",
            "Pred Risk: 0.1170257180929184, True Risk: 0.01107997726649046\n",
            "Pred Risk: 0.08727951347827911, True Risk: 0.009904395788908005\n",
            "Pred Risk: 0.08074282854795456, True Risk: 0.010216973721981049\n",
            "Pred Risk: 0.08470107614994049, True Risk: 0.010494489222764969\n",
            "Pred Risk: 0.08319900929927826, True Risk: 0.009831690229475498\n",
            "Pred Risk: 0.0975634753704071, True Risk: 0.009768140502274036\n",
            "Pred Risk: 0.12102574855089188, True Risk: 0.00995336938649416\n",
            "Pred Risk: 0.1181836724281311, True Risk: 0.010656369850039482\n",
            "Avg Train Excess Risk: 0.07974864542484283, Test Excess Risk: 0.10752730071544647\n",
            "Pred Risk: 0.08547256141901016, True Risk: 0.009883706457912922\n",
            "Pred Risk: 0.08034558594226837, True Risk: 0.009985043667256832\n",
            "Pred Risk: 0.07871697843074799, True Risk: 0.010031601414084435\n",
            "Pred Risk: 0.05225572735071182, True Risk: 0.009886067360639572\n",
            "Pred Risk: 0.1214296817779541, True Risk: 0.010118866339325905\n",
            "Pred Risk: 0.06098397076129913, True Risk: 0.010578330606222153\n",
            "Pred Risk: 0.050017014145851135, True Risk: 0.009790584444999695\n",
            "Pred Risk: 0.06242981553077698, True Risk: 0.010111898183822632\n",
            "Pred Risk: 0.06956552714109421, True Risk: 0.010090149939060211\n",
            "Pred Risk: 0.07434114813804626, True Risk: 0.009782252833247185\n",
            "Pred Risk: 0.09551898390054703, True Risk: 0.009092448279261589\n",
            "Avg Train Excess Risk: 0.06352995336055756, Test Excess Risk: 0.08642653375864029\n",
            "Pred Risk: 0.03446118161082268, True Risk: 0.009843967854976654\n",
            "Pred Risk: 0.04814025014638901, True Risk: 0.010541356168687344\n",
            "Pred Risk: 0.05184982717037201, True Risk: 0.009994760155677795\n",
            "Pred Risk: 0.051936130970716476, True Risk: 0.009754333645105362\n",
            "Pred Risk: 0.03609028831124306, True Risk: 0.010206806473433971\n",
            "Pred Risk: 0.03446787968277931, True Risk: 0.009853214025497437\n",
            "Pred Risk: 0.0495910719037056, True Risk: 0.01023663580417633\n",
            "Pred Risk: 0.0445995070040226, True Risk: 0.009851012378931046\n",
            "Pred Risk: 0.03478814288973808, True Risk: 0.010390358045697212\n",
            "Pred Risk: 0.03293149545788765, True Risk: 0.010226382873952389\n",
            "Pred Risk: 0.05445856600999832, True Risk: 0.009902363643050194\n",
            "Avg Train Excess Risk: 0.0317956916987896, Test Excess Risk: 0.04455620050430298\n",
            "Pred Risk: 0.07199054956436157, True Risk: 0.010305564850568771\n",
            "Pred Risk: 0.05217057839035988, True Risk: 0.009861507453024387\n",
            "Pred Risk: 0.05652521178126335, True Risk: 0.009596333838999271\n",
            "Pred Risk: 0.04883546382188797, True Risk: 0.009919272735714912\n",
            "Pred Risk: 0.06457376480102539, True Risk: 0.010410159826278687\n",
            "Pred Risk: 0.06317826360464096, True Risk: 0.010347677394747734\n",
            "Pred Risk: 0.05471661314368248, True Risk: 0.009965493343770504\n",
            "Pred Risk: 0.06283146888017654, True Risk: 0.009904913604259491\n",
            "Pred Risk: 0.0454491563141346, True Risk: 0.010186766274273396\n",
            "Pred Risk: 0.06657411903142929, True Risk: 0.009778265841305256\n",
            "Pred Risk: 0.09568928182125092, True Risk: 0.010077182203531265\n",
            "Avg Train Excess Risk: 0.048656921833753586, Test Excess Risk: 0.08561210334300995\n",
            "Pred Risk: 0.04943033680319786, True Risk: 0.010036248713731766\n",
            "Pred Risk: 0.07564788311719894, True Risk: 0.009951298125088215\n",
            "Pred Risk: 0.05329083651304245, True Risk: 0.010379289276897907\n",
            "Pred Risk: 0.05795135349035263, True Risk: 0.010417763143777847\n",
            "Pred Risk: 0.08594298362731934, True Risk: 0.010357704013586044\n",
            "Pred Risk: 0.0748431384563446, True Risk: 0.00929129309952259\n",
            "Pred Risk: 0.06568218767642975, True Risk: 0.009549636393785477\n",
            "Pred Risk: 0.0603078156709671, True Risk: 0.00961288996040821\n",
            "Pred Risk: 0.08152375370264053, True Risk: 0.010357626713812351\n",
            "Pred Risk: 0.06348956376314163, True Risk: 0.010515118017792702\n",
            "Pred Risk: 0.11165161430835724, True Risk: 0.010467497631907463\n",
            "Avg Train Excess Risk: 0.05676410347223282, Test Excess Risk: 0.10118411481380463\n",
            "Pred Risk: 0.05411343276500702, True Risk: 0.009867648594081402\n",
            "Pred Risk: 0.03914722427725792, True Risk: 0.010335886850953102\n",
            "Pred Risk: 0.057178810238838196, True Risk: 0.0095693813636899\n",
            "Pred Risk: 0.06719178706407547, True Risk: 0.01020294800400734\n",
            "Pred Risk: 0.047123756259679794, True Risk: 0.010150386020541191\n",
            "Pred Risk: 0.0581132136285305, True Risk: 0.00986510794609785\n",
            "Pred Risk: 0.04944043979048729, True Risk: 0.00940129067748785\n",
            "Pred Risk: 0.050552163273096085, True Risk: 0.010496068745851517\n",
            "Pred Risk: 0.04643534868955612, True Risk: 0.00993316899985075\n",
            "Pred Risk: 0.07249685376882553, True Risk: 0.01039035338908434\n",
            "Pred Risk: 0.08566278964281082, True Risk: 0.010377006605267525\n",
            "Avg Train Excess Risk: 0.044158078730106354, Test Excess Risk: 0.07528578490018845\n",
            "Pred Risk: 0.13118503987789154, True Risk: 0.009774476289749146\n",
            "Pred Risk: 0.08059876412153244, True Risk: 0.010061078704893589\n",
            "Pred Risk: 0.11659026890993118, True Risk: 0.01000205148011446\n",
            "Pred Risk: 0.10909043997526169, True Risk: 0.009851442649960518\n",
            "Pred Risk: 0.13269971311092377, True Risk: 0.01016500499099493\n",
            "Pred Risk: 0.1450955718755722, True Risk: 0.010170813649892807\n",
            "Pred Risk: 0.09986358880996704, True Risk: 0.009717714041471481\n",
            "Pred Risk: 0.11186468601226807, True Risk: 0.009680688381195068\n",
            "Pred Risk: 0.06734393537044525, True Risk: 0.010086419060826302\n",
            "Pred Risk: 0.0844629555940628, True Risk: 0.010018637403845787\n",
            "Pred Risk: 0.11169698089361191, True Risk: 0.009321644902229309\n",
            "Avg Train Excess Risk: 0.09792666137218475, Test Excess Risk: 0.1023753359913826\n",
            "Pred Risk: 0.06767614185810089, True Risk: 0.010095184668898582\n",
            "Pred Risk: 0.08726738393306732, True Risk: 0.010495457798242569\n",
            "Pred Risk: 0.05454166233539581, True Risk: 0.010674435645341873\n",
            "Pred Risk: 0.0956861600279808, True Risk: 0.010036765597760677\n",
            "Pred Risk: 0.06792173534631729, True Risk: 0.00969694647938013\n",
            "Pred Risk: 0.08417504280805588, True Risk: 0.009997726418077946\n",
            "Pred Risk: 0.08404432237148285, True Risk: 0.010666931048035622\n",
            "Pred Risk: 0.08650029450654984, True Risk: 0.009856879711151123\n",
            "Pred Risk: 0.05567790940403938, True Risk: 0.009939969517290592\n",
            "Pred Risk: 0.08324901759624481, True Risk: 0.01008232869207859\n",
            "Pred Risk: 0.11213336884975433, True Risk: 0.010814886540174484\n",
            "Avg Train Excess Risk: 0.06651969999074936, Test Excess Risk: 0.10131847858428955\n",
            "Pred Risk: 0.05129178985953331, True Risk: 0.009938557632267475\n",
            "Pred Risk: 0.06766288727521896, True Risk: 0.010183251462876797\n",
            "Pred Risk: 0.0508308969438076, True Risk: 0.010705039836466312\n",
            "Pred Risk: 0.09619108587503433, True Risk: 0.009657340124249458\n",
            "Pred Risk: 0.063494972884655, True Risk: 0.010269906371831894\n",
            "Pred Risk: 0.0612635537981987, True Risk: 0.010145941749215126\n",
            "Pred Risk: 0.06447818875312805, True Risk: 0.00992055144160986\n",
            "Pred Risk: 0.08485054969787598, True Risk: 0.01007749978452921\n",
            "Pred Risk: 0.06434547156095505, True Risk: 0.010271082632243633\n",
            "Pred Risk: 0.06549552083015442, True Risk: 0.010385841131210327\n",
            "Pred Risk: 0.07984889298677444, True Risk: 0.010120709426701069\n",
            "Avg Train Excess Risk: 0.05683499574661255, Test Excess Risk: 0.06972818076610565\n",
            "Pred Risk: 0.0896284207701683, True Risk: 0.010744607076048851\n",
            "Pred Risk: 0.08672899752855301, True Risk: 0.009845403954386711\n",
            "Pred Risk: 0.07423926889896393, True Risk: 0.009972913190722466\n",
            "Pred Risk: 0.052885036915540695, True Risk: 0.010182353667914867\n",
            "Pred Risk: 0.06651905179023743, True Risk: 0.009567552246153355\n",
            "Pred Risk: 0.08808407932519913, True Risk: 0.010342522524297237\n",
            "Pred Risk: 0.08642280101776123, True Risk: 0.009264020249247551\n",
            "Pred Risk: 0.08364808559417725, True Risk: 0.00977242924273014\n",
            "Pred Risk: 0.06375031918287277, True Risk: 0.009886888787150383\n",
            "Pred Risk: 0.08703962713479996, True Risk: 0.008981871418654919\n",
            "Pred Risk: 0.13210292160511017, True Risk: 0.009569283574819565\n",
            "Avg Train Excess Risk: 0.06803850829601288, Test Excess Risk: 0.1225336343050003\n",
            "Pred Risk: 0.07412617653608322, True Risk: 0.010367803275585175\n",
            "Pred Risk: 0.05362870544195175, True Risk: 0.009798578917980194\n",
            "Pred Risk: 0.04968797415494919, True Risk: 0.010051141493022442\n",
            "Pred Risk: 0.06287812441587448, True Risk: 0.0098783690482378\n",
            "Pred Risk: 0.07058054208755493, True Risk: 0.010087541304528713\n",
            "Pred Risk: 0.05032920092344284, True Risk: 0.009633157402276993\n",
            "Pred Risk: 0.05391078442335129, True Risk: 0.009506044909358025\n",
            "Pred Risk: 0.08255255222320557, True Risk: 0.00977028626948595\n",
            "Pred Risk: 0.07342254370450974, True Risk: 0.01019230205565691\n",
            "Pred Risk: 0.08649792522192001, True Risk: 0.010202986188232899\n",
            "Pred Risk: 0.12158634513616562, True Risk: 0.01015863660722971\n",
            "Avg Train Excess Risk: 0.05581263452768326, Test Excess Risk: 0.11142770946025848\n",
            "Pred Risk: 0.03833684325218201, True Risk: 0.010028601624071598\n",
            "Pred Risk: 0.08365621417760849, True Risk: 0.009781692177057266\n",
            "Pred Risk: 0.08326098322868347, True Risk: 0.010494384914636612\n",
            "Pred Risk: 0.07612092792987823, True Risk: 0.010412817820906639\n",
            "Pred Risk: 0.0787258967757225, True Risk: 0.010064709931612015\n",
            "Pred Risk: 0.09771935641765594, True Risk: 0.010409186594188213\n",
            "Pred Risk: 0.07454301416873932, True Risk: 0.010044578462839127\n",
            "Pred Risk: 0.06134101375937462, True Risk: 0.010310864076018333\n",
            "Pred Risk: 0.07408615946769714, True Risk: 0.009118415415287018\n",
            "Pred Risk: 0.07309898734092712, True Risk: 0.009976468048989773\n",
            "Pred Risk: 0.10027138143777847, True Risk: 0.010398627258837223\n",
            "Avg Train Excess Risk: 0.06402476131916046, Test Excess Risk: 0.08987275511026382\n",
            "Pred Risk: 0.06829190999269485, True Risk: 0.01041311863809824\n",
            "Pred Risk: 0.05913387984037399, True Risk: 0.010062355548143387\n",
            "Pred Risk: 0.054648563265800476, True Risk: 0.009896225295960903\n",
            "Pred Risk: 0.06854581832885742, True Risk: 0.009812755510210991\n",
            "Pred Risk: 0.05063284561038017, True Risk: 0.009878489188849926\n",
            "Pred Risk: 0.05442344769835472, True Risk: 0.009808385744690895\n",
            "Pred Risk: 0.04216756671667099, True Risk: 0.009820478036999702\n",
            "Pred Risk: 0.07304349541664124, True Risk: 0.010074990801513195\n",
            "Pred Risk: 0.058263570070266724, True Risk: 0.009357362054288387\n",
            "Pred Risk: 0.05279140546917915, True Risk: 0.009827916510403156\n",
            "Pred Risk: 0.08041587471961975, True Risk: 0.009984214790165424\n",
            "Avg Train Excess Risk: 0.04829903692007065, Test Excess Risk: 0.0704316571354866\n",
            "Pred Risk: 0.06051494553685188, True Risk: 0.009433374740183353\n",
            "Pred Risk: 0.05250678211450577, True Risk: 0.01072680577635765\n",
            "Pred Risk: 0.0617351271212101, True Risk: 0.010074055753648281\n",
            "Pred Risk: 0.05730723589658737, True Risk: 0.010159261524677277\n",
            "Pred Risk: 0.05164527893066406, True Risk: 0.010508544743061066\n",
            "Pred Risk: 0.072283536195755, True Risk: 0.010151749476790428\n",
            "Pred Risk: 0.06758230179548264, True Risk: 0.009212317876517773\n",
            "Pred Risk: 0.07684455811977386, True Risk: 0.01030747964978218\n",
            "Pred Risk: 0.049413591623306274, True Risk: 0.00996042788028717\n",
            "Pred Risk: 0.0864034965634346, True Risk: 0.009341513738036156\n",
            "Pred Risk: 0.0768248662352562, True Risk: 0.010170212015509605\n",
            "Avg Train Excess Risk: 0.053636133670806885, Test Excess Risk: 0.06665465235710144\n",
            "Pred Risk: 0.056671276688575745, True Risk: 0.009805194102227688\n",
            "Pred Risk: 0.05486111342906952, True Risk: 0.01013962458819151\n",
            "Pred Risk: 0.03987389802932739, True Risk: 0.01002460066229105\n",
            "Pred Risk: 0.053996920585632324, True Risk: 0.009673652239143848\n",
            "Pred Risk: 0.06027443706989288, True Risk: 0.010711204260587692\n",
            "Pred Risk: 0.06398767977952957, True Risk: 0.00989385973662138\n",
            "Pred Risk: 0.04958782717585564, True Risk: 0.01026590634137392\n",
            "Pred Risk: 0.059742458164691925, True Risk: 0.010264677926898003\n",
            "Pred Risk: 0.05504494532942772, True Risk: 0.009413634426891804\n",
            "Pred Risk: 0.048880357295274734, True Risk: 0.00998650398105383\n",
            "Pred Risk: 0.08642987906932831, True Risk: 0.009456180967390537\n",
            "Avg Train Excess Risk: 0.04427420347929001, Test Excess Risk: 0.07697369903326035\n",
            "Pred Risk: 0.07115142047405243, True Risk: 0.009511268697679043\n",
            "Pred Risk: 0.08653842657804489, True Risk: 0.01026079524308443\n",
            "Pred Risk: 0.053674958646297455, True Risk: 0.009691775776445866\n",
            "Pred Risk: 0.08487441390752792, True Risk: 0.010204999707639217\n",
            "Pred Risk: 0.09162849187850952, True Risk: 0.010340980254113674\n",
            "Pred Risk: 0.08845921605825424, True Risk: 0.009756725281476974\n",
            "Pred Risk: 0.07619649171829224, True Risk: 0.01070621982216835\n",
            "Pred Risk: 0.07468821108341217, True Risk: 0.010415246710181236\n",
            "Pred Risk: 0.0957312062382698, True Risk: 0.009795745834708214\n",
            "Pred Risk: 0.0883999913930893, True Risk: 0.009725898504257202\n",
            "Pred Risk: 0.1552082598209381, True Risk: 0.00899177324026823\n",
            "Avg Train Excess Risk: 0.07109331339597702, Test Excess Risk: 0.146216481924057\n",
            "Pred Risk: 0.08161886036396027, True Risk: 0.010098612867295742\n",
            "Pred Risk: 0.06363941729068756, True Risk: 0.010140872560441494\n",
            "Pred Risk: 0.06828510761260986, True Risk: 0.010872949846088886\n",
            "Pred Risk: 0.07036054879426956, True Risk: 0.010533314198255539\n",
            "Pred Risk: 0.08640538156032562, True Risk: 0.01005912572145462\n",
            "Pred Risk: 0.0732613131403923, True Risk: 0.010339350439608097\n",
            "Pred Risk: 0.07537353783845901, True Risk: 0.0100043173879385\n",
            "Pred Risk: 0.06852317601442337, True Risk: 0.009459855034947395\n",
            "Pred Risk: 0.07533028721809387, True Risk: 0.010392330586910248\n",
            "Pred Risk: 0.07679493725299835, True Risk: 0.010121733881533146\n",
            "Pred Risk: 0.09832345694303513, True Risk: 0.009498858824372292\n",
            "Avg Train Excess Risk: 0.06375700235366821, Test Excess Risk: 0.08882459998130798\n",
            "Pred Risk: 0.05132538452744484, True Risk: 0.00986183900386095\n",
            "Pred Risk: 0.05977489426732063, True Risk: 0.010563336312770844\n",
            "Pred Risk: 0.0670022964477539, True Risk: 0.00980356615036726\n",
            "Pred Risk: 0.08615417778491974, True Risk: 0.009430227801203728\n",
            "Pred Risk: 0.08915960788726807, True Risk: 0.010093645192682743\n",
            "Pred Risk: 0.0910584032535553, True Risk: 0.009964815340936184\n",
            "Pred Risk: 0.07029037177562714, True Risk: 0.010168427601456642\n",
            "Pred Risk: 0.08051244169473648, True Risk: 0.009896096773445606\n",
            "Pred Risk: 0.06754264235496521, True Risk: 0.010063915513455868\n",
            "Pred Risk: 0.10628008097410202, True Risk: 0.009411218576133251\n",
            "Pred Risk: 0.08198190480470657, True Risk: 0.00986439734697342\n",
            "Avg Train Excess Risk: 0.06698433309793472, Test Excess Risk: 0.07211750745773315\n",
            "Pred Risk: 0.1126943826675415, True Risk: 0.009709236212074757\n",
            "Pred Risk: 0.08023513853549957, True Risk: 0.010085169225931168\n",
            "Pred Risk: 0.0770261138677597, True Risk: 0.009904343634843826\n",
            "Pred Risk: 0.09748837351799011, True Risk: 0.010209200903773308\n",
            "Pred Risk: 0.07492894679307938, True Risk: 0.009764011017978191\n",
            "Pred Risk: 0.06499754637479782, True Risk: 0.009807387366890907\n",
            "Pred Risk: 0.07048607617616653, True Risk: 0.009763356298208237\n",
            "Pred Risk: 0.07022066414356232, True Risk: 0.010003570467233658\n",
            "Pred Risk: 0.06990380585193634, True Risk: 0.010352130979299545\n",
            "Pred Risk: 0.06661274284124374, True Risk: 0.010944772511720657\n",
            "Pred Risk: 0.09788749366998672, True Risk: 0.010503270663321018\n",
            "Avg Train Excess Risk: 0.06840505450963974, Test Excess Risk: 0.08738422393798828\n",
            "Pred Risk: 0.05870693176984787, True Risk: 0.010015006177127361\n",
            "Pred Risk: 0.0628129169344902, True Risk: 0.009531673975288868\n",
            "Pred Risk: 0.047341760247945786, True Risk: 0.009815769270062447\n",
            "Pred Risk: 0.036469027400016785, True Risk: 0.010148848406970501\n",
            "Pred Risk: 0.06250934302806854, True Risk: 0.01017290074378252\n",
            "Pred Risk: 0.06419098377227783, True Risk: 0.009467842057347298\n",
            "Pred Risk: 0.06034873425960541, True Risk: 0.009970786981284618\n",
            "Pred Risk: 0.05700398609042168, True Risk: 0.00977510679513216\n",
            "Pred Risk: 0.05521617829799652, True Risk: 0.01045935321599245\n",
            "Pred Risk: 0.03989033028483391, True Risk: 0.00987965241074562\n",
            "Pred Risk: 0.06780374050140381, True Risk: 0.009935496374964714\n",
            "Avg Train Excess Risk: 0.04452532157301903, Test Excess Risk: 0.057868242263793945\n",
            "Pred Risk: 0.036241017282009125, True Risk: 0.010550828650593758\n",
            "Pred Risk: 0.06058170273900032, True Risk: 0.01006610319018364\n",
            "Pred Risk: 0.0520063191652298, True Risk: 0.00980046484619379\n",
            "Pred Risk: 0.05903559550642967, True Risk: 0.009680259972810745\n",
            "Pred Risk: 0.05693994462490082, True Risk: 0.009859863668680191\n",
            "Pred Risk: 0.05784935504198074, True Risk: 0.009855161421000957\n",
            "Pred Risk: 0.05206986889243126, True Risk: 0.00994859728962183\n",
            "Pred Risk: 0.03765062987804413, True Risk: 0.009924604557454586\n",
            "Pred Risk: 0.048845864832401276, True Risk: 0.009987691417336464\n",
            "Pred Risk: 0.04724496230483055, True Risk: 0.01055480632930994\n",
            "Pred Risk: 0.09091824293136597, True Risk: 0.00975609291344881\n",
            "Avg Train Excess Risk: 0.04082368686795235, Test Excess Risk: 0.08116214722394943\n",
            "Pred Risk: 0.07277620583772659, True Risk: 0.009874441660940647\n",
            "Pred Risk: 0.05351690948009491, True Risk: 0.010330730117857456\n",
            "Pred Risk: 0.04936344921588898, True Risk: 0.009488127194344997\n",
            "Pred Risk: 0.06418319791555405, True Risk: 0.010074254125356674\n",
            "Pred Risk: 0.05404307693243027, True Risk: 0.00949957687407732\n",
            "Pred Risk: 0.0512617826461792, True Risk: 0.009879713878035545\n",
            "Pred Risk: 0.06306511163711548, True Risk: 0.00984775461256504\n",
            "Pred Risk: 0.056585680693387985, True Risk: 0.010336495004594326\n",
            "Pred Risk: 0.06090191379189491, True Risk: 0.010404917411506176\n",
            "Pred Risk: 0.07495827972888947, True Risk: 0.00945548340678215\n",
            "Pred Risk: 0.09814996272325516, True Risk: 0.009818324819207191\n",
            "Avg Train Excess Risk: 0.05014641210436821, Test Excess Risk: 0.08833163976669312\n",
            "Pred Risk: 0.05679849162697792, True Risk: 0.010204590857028961\n",
            "Pred Risk: 0.04507439583539963, True Risk: 0.009695158340036869\n",
            "Pred Risk: 0.05885421112179756, True Risk: 0.010528096929192543\n",
            "Pred Risk: 0.06740404665470123, True Risk: 0.009601791389286518\n",
            "Pred Risk: 0.05886195972561836, True Risk: 0.009289538487792015\n",
            "Pred Risk: 0.05669885501265526, True Risk: 0.009864671155810356\n",
            "Pred Risk: 0.05658458545804024, True Risk: 0.009517074562609196\n",
            "Pred Risk: 0.0609852597117424, True Risk: 0.01004832610487938\n",
            "Pred Risk: 0.04304886981844902, True Risk: 0.010333691723644733\n",
            "Pred Risk: 0.055146828293800354, True Risk: 0.01034477911889553\n",
            "Pred Risk: 0.08918929100036621, True Risk: 0.009673111140727997\n",
            "Avg Train Excess Risk: 0.04600297659635544, Test Excess Risk: 0.07951617985963821\n",
            "Pred Risk: 0.07165428251028061, True Risk: 0.009604986757040024\n",
            "Pred Risk: 0.07337034493684769, True Risk: 0.010617533698678017\n",
            "Pred Risk: 0.08041875809431076, True Risk: 0.009622070007026196\n",
            "Pred Risk: 0.0763213261961937, True Risk: 0.010121362283825874\n",
            "Pred Risk: 0.07436951994895935, True Risk: 0.009732922539114952\n",
            "Pred Risk: 0.05158486217260361, True Risk: 0.010163610801100731\n",
            "Pred Risk: 0.11657509207725525, True Risk: 0.009681109338998795\n",
            "Pred Risk: 0.07716947048902512, True Risk: 0.009607084095478058\n",
            "Pred Risk: 0.08419379591941833, True Risk: 0.010456346906721592\n",
            "Pred Risk: 0.06102367490530014, True Risk: 0.009754392318427563\n",
            "Pred Risk: 0.10804079473018646, True Risk: 0.009993065148591995\n",
            "Avg Train Excess Risk: 0.06673197448253632, Test Excess Risk: 0.09804773330688477\n",
            "Pred Risk: 0.06174212321639061, True Risk: 0.009784362278878689\n",
            "Pred Risk: 0.05672207474708557, True Risk: 0.009901045821607113\n",
            "Pred Risk: 0.06440302729606628, True Risk: 0.0091098016127944\n",
            "Pred Risk: 0.07721206545829773, True Risk: 0.009377498179674149\n",
            "Pred Risk: 0.04676184803247452, True Risk: 0.00980016402900219\n",
            "Pred Risk: 0.06805578619241714, True Risk: 0.009940488263964653\n",
            "Pred Risk: 0.05255233123898506, True Risk: 0.010399364866316319\n",
            "Pred Risk: 0.055566489696502686, True Risk: 0.009503097273409367\n",
            "Pred Risk: 0.05821583420038223, True Risk: 0.010431379079818726\n",
            "Pred Risk: 0.05615778639912605, True Risk: 0.010646017268300056\n",
            "Pred Risk: 0.07419649511575699, True Risk: 0.009837977588176727\n",
            "Avg Train Excess Risk: 0.04984961822628975, Test Excess Risk: 0.06435851752758026\n",
            "Pred Risk: 0.08247186243534088, True Risk: 0.010249064303934574\n",
            "Pred Risk: 0.06972785294055939, True Risk: 0.00961352325975895\n",
            "Pred Risk: 0.07095733284950256, True Risk: 0.009931853972375393\n",
            "Pred Risk: 0.06979937851428986, True Risk: 0.010193007066845894\n",
            "Pred Risk: 0.06337102502584457, True Risk: 0.010217253118753433\n",
            "Pred Risk: 0.07486215978860855, True Risk: 0.009862508624792099\n",
            "Pred Risk: 0.06484188139438629, True Risk: 0.009401988238096237\n",
            "Pred Risk: 0.05798253417015076, True Risk: 0.010336927138268948\n",
            "Pred Risk: 0.09990280866622925, True Risk: 0.009636389091610909\n",
            "Pred Risk: 0.07677503675222397, True Risk: 0.010210067965090275\n",
            "Pred Risk: 0.08643055707216263, True Risk: 0.009976950474083424\n",
            "Avg Train Excess Risk: 0.06310392916202545, Test Excess Risk: 0.07645360380411148\n",
            "Pred Risk: 0.07729961723089218, True Risk: 0.01038389466702938\n",
            "Pred Risk: 0.07258746027946472, True Risk: 0.010316002182662487\n",
            "Pred Risk: 0.06046995148062706, True Risk: 0.01052253320813179\n",
            "Pred Risk: 0.07796298712491989, True Risk: 0.009962116368114948\n",
            "Pred Risk: 0.09209661185741425, True Risk: 0.010098476894199848\n",
            "Pred Risk: 0.08805637061595917, True Risk: 0.00979534164071083\n",
            "Pred Risk: 0.06981043517589569, True Risk: 0.009815744124352932\n",
            "Pred Risk: 0.07125669717788696, True Risk: 0.009591416455805302\n",
            "Pred Risk: 0.07210403680801392, True Risk: 0.010655412450432777\n",
            "Pred Risk: 0.08249389380216599, True Risk: 0.010109219700098038\n",
            "Pred Risk: 0.10601017624139786, True Risk: 0.009614793583750725\n",
            "Avg Train Excess Risk: 0.0662887915968895, Test Excess Risk: 0.09639538079500198\n",
            "Pred Risk: 0.05630958080291748, True Risk: 0.010284294374287128\n",
            "Pred Risk: 0.060620810836553574, True Risk: 0.009915202856063843\n",
            "Pred Risk: 0.08214286714792252, True Risk: 0.009638727642595768\n",
            "Pred Risk: 0.04782523214817047, True Risk: 0.009182067587971687\n",
            "Pred Risk: 0.07665109634399414, True Risk: 0.010546679608523846\n",
            "Pred Risk: 0.07374022156000137, True Risk: 0.00983582716435194\n",
            "Pred Risk: 0.06958220154047012, True Risk: 0.010225087404251099\n",
            "Pred Risk: 0.07394890487194061, True Risk: 0.009605168364942074\n",
            "Pred Risk: 0.06453002989292145, True Risk: 0.009712956845760345\n",
            "Pred Risk: 0.063785620033741, True Risk: 0.00979869905859232\n",
            "Pred Risk: 0.10489846765995026, True Risk: 0.009723508730530739\n",
            "Avg Train Excess Risk: 0.05703919008374214, Test Excess Risk: 0.09517496079206467\n",
            "Pred Risk: 0.07608389109373093, True Risk: 0.010399370454251766\n",
            "Pred Risk: 0.07968050241470337, True Risk: 0.010218908078968525\n",
            "Pred Risk: 0.07316139340400696, True Risk: 0.010377071797847748\n",
            "Pred Risk: 0.0995527058839798, True Risk: 0.009202375076711178\n",
            "Pred Risk: 0.07208807021379471, True Risk: 0.010231290012598038\n",
            "Pred Risk: 0.06190827861428261, True Risk: 0.009367612190544605\n",
            "Pred Risk: 0.06135021150112152, True Risk: 0.009845438413321972\n",
            "Pred Risk: 0.1036486029624939, True Risk: 0.0097120963037014\n",
            "Pred Risk: 0.07601422071456909, True Risk: 0.00957830436527729\n",
            "Pred Risk: 0.07737469673156738, True Risk: 0.009709595702588558\n",
            "Pred Risk: 0.1066628023982048, True Risk: 0.009844806045293808\n",
            "Avg Train Excess Risk: 0.0682220533490181, Test Excess Risk: 0.0968180000782013\n",
            "Pred Risk: 0.10688205808401108, True Risk: 0.010298115201294422\n",
            "Pred Risk: 0.09885334968566895, True Risk: 0.010359540581703186\n",
            "Pred Risk: 0.07712337374687195, True Risk: 0.011004215106368065\n",
            "Pred Risk: 0.11697307229042053, True Risk: 0.009531153365969658\n",
            "Pred Risk: 0.09048284590244293, True Risk: 0.0099095543846488\n",
            "Pred Risk: 0.10490098595619202, True Risk: 0.009834588505327702\n",
            "Pred Risk: 0.09487128257751465, True Risk: 0.009559071622788906\n",
            "Pred Risk: 0.06118180602788925, True Risk: 0.009966208599507809\n",
            "Pred Risk: 0.07297419011592865, True Risk: 0.009926007129251957\n",
            "Pred Risk: 0.07830054312944412, True Risk: 0.0097388606518507\n",
            "Pred Risk: 0.16707059741020203, True Risk: 0.009559034369885921\n",
            "Avg Train Excess Risk: 0.0802416205406189, Test Excess Risk: 0.15751156210899353\n",
            "Pred Risk: 0.05931958556175232, True Risk: 0.009530569426715374\n",
            "Pred Risk: 0.06030904874205589, True Risk: 0.010108919814229012\n",
            "Pred Risk: 0.056311048567295074, True Risk: 0.010524033568799496\n",
            "Pred Risk: 0.05794459581375122, True Risk: 0.009622347541153431\n",
            "Pred Risk: 0.06780090183019638, True Risk: 0.010288565419614315\n",
            "Pred Risk: 0.05436617508530617, True Risk: 0.009541386738419533\n",
            "Pred Risk: 0.05140845105051994, True Risk: 0.009877421893179417\n",
            "Pred Risk: 0.06716983765363693, True Risk: 0.009873616509139538\n",
            "Pred Risk: 0.055654559284448624, True Risk: 0.010432248935103416\n",
            "Pred Risk: 0.06496530771255493, True Risk: 0.009894736111164093\n",
            "Pred Risk: 0.0714467242360115, True Risk: 0.009916428476572037\n",
            "Avg Train Excess Risk: 0.049555566161870956, Test Excess Risk: 0.06153029575943947\n",
            "Pred Risk: 0.06732818484306335, True Risk: 0.010052463971078396\n",
            "Pred Risk: 0.09081175923347473, True Risk: 0.009846007451415062\n",
            "Pred Risk: 0.09087003767490387, True Risk: 0.009977036155760288\n",
            "Pred Risk: 0.10791195183992386, True Risk: 0.009841186925768852\n",
            "Pred Risk: 0.0588713139295578, True Risk: 0.009919428266584873\n",
            "Pred Risk: 0.059105753898620605, True Risk: 0.009817217476665974\n",
            "Pred Risk: 0.04623563587665558, True Risk: 0.009903217665851116\n",
            "Pred Risk: 0.09169477969408035, True Risk: 0.011033952236175537\n",
            "Pred Risk: 0.08131712675094604, True Risk: 0.010019122622907162\n",
            "Pred Risk: 0.09834615886211395, True Risk: 0.010058240965008736\n",
            "Pred Risk: 0.1234908327460289, True Risk: 0.010085108689963818\n",
            "Avg Train Excess Risk: 0.0692024827003479, Test Excess Risk: 0.1134057268500328\n",
            "Pred Risk: 0.10728852450847626, True Risk: 0.010032359510660172\n",
            "Pred Risk: 0.07549556344747543, True Risk: 0.009752152487635612\n",
            "Pred Risk: 0.07895245403051376, True Risk: 0.010659207589924335\n",
            "Pred Risk: 0.07309671491384506, True Risk: 0.01031474582850933\n",
            "Pred Risk: 0.0488005168735981, True Risk: 0.010210956446826458\n",
            "Pred Risk: 0.06999870389699936, True Risk: 0.009510216303169727\n",
            "Pred Risk: 0.08853143453598022, True Risk: 0.010143888182938099\n",
            "Pred Risk: 0.07635486871004105, True Risk: 0.010329722426831722\n",
            "Pred Risk: 0.10522580146789551, True Risk: 0.009696464985609055\n",
            "Pred Risk: 0.06240581348538399, True Risk: 0.009720748290419579\n",
            "Pred Risk: 0.13818763196468353, True Risk: 0.009866015985608101\n",
            "Avg Train Excess Risk: 0.06857798993587494, Test Excess Risk: 0.12832161784172058\n",
            "Pred Risk: 0.06991879642009735, True Risk: 0.010148566216230392\n",
            "Pred Risk: 0.059281934052705765, True Risk: 0.010817352682352066\n",
            "Pred Risk: 0.05769950896501541, True Risk: 0.009580176323652267\n",
            "Pred Risk: 0.0607113279402256, True Risk: 0.009731223806738853\n",
            "Pred Risk: 0.06559295952320099, True Risk: 0.010354825295507908\n",
            "Pred Risk: 0.05175234377384186, True Risk: 0.010634936392307281\n",
            "Pred Risk: 0.05205526202917099, True Risk: 0.009918984957039356\n",
            "Pred Risk: 0.05732118710875511, True Risk: 0.01029428280889988\n",
            "Pred Risk: 0.05380261689424515, True Risk: 0.009601399302482605\n",
            "Pred Risk: 0.0668344721198082, True Risk: 0.010196663439273834\n",
            "Pred Risk: 0.08493738621473312, True Risk: 0.010538410395383835\n",
            "Avg Train Excess Risk: 0.0493692010641098, Test Excess Risk: 0.07439897954463959\n",
            "Pred Risk: 0.062109146267175674, True Risk: 0.009335166774690151\n",
            "Pred Risk: 0.06021854281425476, True Risk: 0.010079405270516872\n",
            "Pred Risk: 0.053298916667699814, True Risk: 0.009798654355108738\n",
            "Pred Risk: 0.05785350874066353, True Risk: 0.00945884920656681\n",
            "Pred Risk: 0.05535926669836044, True Risk: 0.010786676779389381\n",
            "Pred Risk: 0.06132306531071663, True Risk: 0.009854696691036224\n",
            "Pred Risk: 0.06288637220859528, True Risk: 0.009918254800140858\n",
            "Pred Risk: 0.07629617303609848, True Risk: 0.010095411911606789\n",
            "Pred Risk: 0.05258144810795784, True Risk: 0.011074822396039963\n",
            "Pred Risk: 0.06974659115076065, True Risk: 0.010649451985955238\n",
            "Pred Risk: 0.08079259097576141, True Risk: 0.009614736773073673\n",
            "Avg Train Excess Risk: 0.051062166690826416, Test Excess Risk: 0.07117785513401031\n",
            "Pred Risk: 0.04582410678267479, True Risk: 0.010430499911308289\n",
            "Pred Risk: 0.05235259607434273, True Risk: 0.010304917581379414\n",
            "Pred Risk: 0.05567492917180061, True Risk: 0.009935274720191956\n",
            "Pred Risk: 0.052802134305238724, True Risk: 0.009880157187581062\n",
            "Pred Risk: 0.05008058622479439, True Risk: 0.010231910273432732\n",
            "Pred Risk: 0.07453328371047974, True Risk: 0.009928768500685692\n",
            "Pred Risk: 0.048802103847265244, True Risk: 0.00991814024746418\n",
            "Pred Risk: 0.049338676035404205, True Risk: 0.010923165827989578\n",
            "Pred Risk: 0.05727377533912659, True Risk: 0.010481695644557476\n",
            "Pred Risk: 0.04456665739417076, True Risk: 0.009767986834049225\n",
            "Pred Risk: 0.09694769978523254, True Risk: 0.010207900777459145\n",
            "Avg Train Excess Risk: 0.04294463247060776, Test Excess Risk: 0.08673980087041855\n",
            "Pred Risk: 0.07028200477361679, True Risk: 0.009795613586902618\n",
            "Pred Risk: 0.06620952486991882, True Risk: 0.010393272154033184\n",
            "Pred Risk: 0.0620683878660202, True Risk: 0.009848910383880138\n",
            "Pred Risk: 0.091665118932724, True Risk: 0.009661304764449596\n",
            "Pred Risk: 0.07523451000452042, True Risk: 0.010045388713479042\n",
            "Pred Risk: 0.10115686058998108, True Risk: 0.00941154733300209\n",
            "Pred Risk: 0.06732049584388733, True Risk: 0.00978691503405571\n",
            "Pred Risk: 0.08783574402332306, True Risk: 0.009914024733006954\n",
            "Pred Risk: 0.09659931063652039, True Risk: 0.01056433655321598\n",
            "Pred Risk: 0.07019849121570587, True Risk: 0.01037262100726366\n",
            "Pred Risk: 0.09628785401582718, True Risk: 0.009907662868499756\n",
            "Avg Train Excess Risk: 0.06887765228748322, Test Excess Risk: 0.08638019114732742\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "dx, dy, r = 50, 15, 5\n",
        "num_tasks = 10\n",
        "num_trials = 100\n",
        "n = 100\n",
        "\n",
        "\n",
        "lr = 0.01\n",
        "num_steps = 1000\n",
        "\n",
        "all_transfer_coefficients = []\n",
        "\n",
        "# Run multiple trials\n",
        "for trial in tqdm(range(num_trials), desc=\"Running Experiments\"):\n",
        "    Fs, Phi_star, Sigma_x = generate_parameters(dx, dy, r, num_tasks)\n",
        "    transfer_coefficient = experiment(dx, dy, r, num_tasks, n, lr, num_steps)\n",
        "    all_transfer_coefficients.append(transfer_coefficient)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "2Cs4cQ1SwGud",
        "outputId": "690e0f90-f7c3-46f5-8e11-d654cf3b52b0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "all_transfer_coefficients = torch.tensor(all_transfer_coefficients)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(all_transfer_coefficients.cpu().numpy(), bins=20, alpha=0.7, color=\"blue\")\n",
        "plt.xlabel(\"Transfer Coefficient ()\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(f\"Histogram of Transfer Coefficients (n={n}, Trials={num_trials})\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "70c251c1a1c4fe5052d432358644ac2f1a0638b040fdd112103cd1acb9abe5c8"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5251ced233a84c37a0e4577e214c3f0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ddae83910084e988f0de53cbe4df5e1",
              "IPY_MODEL_9e9b33b2e60043aa94cd47c196d6735a",
              "IPY_MODEL_85608f23de774808aa847b69fad6b511"
            ],
            "layout": "IPY_MODEL_080203714a734a5fa7217982f6d8c603"
          }
        },
        "2ddae83910084e988f0de53cbe4df5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d591723f791401dbbf754ddb3ada469",
            "placeholder": "",
            "style": "IPY_MODEL_0db33db372044da7aa990eec1b467831",
            "value": "RunningExperiments:100%"
          }
        },
        "9e9b33b2e60043aa94cd47c196d6735a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a40d96dfd4bb44b68db5f602739fb87f",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c625ed7aff794045bd56277c2da57413",
            "value": 100
          }
        },
        "85608f23de774808aa847b69fad6b511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c4cb02dca5d467bb386cf3f63e71169",
            "placeholder": "",
            "style": "IPY_MODEL_1f8ffc308a324c33bdcb07579ecd00d0",
            "value": "100/100[17:07&lt;00:00,10.16s/it]"
          }
        },
        "080203714a734a5fa7217982f6d8c603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d591723f791401dbbf754ddb3ada469": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0db33db372044da7aa990eec1b467831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a40d96dfd4bb44b68db5f602739fb87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c625ed7aff794045bd56277c2da57413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c4cb02dca5d467bb386cf3f63e71169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f8ffc308a324c33bdcb07579ecd00d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}